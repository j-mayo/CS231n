{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "softmax.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j-mayo/CS231n/blob/main/softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0MuSenRvmtk",
        "outputId": "2b76ad0c-44d4-4665-c016-8697d269fef2"
      },
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Enter the foldername in your Drive where you have saved the unzipped\n",
        "# assignment folder, e.g. 'cs231n/assignments/assignment1/'\n",
        "FOLDERNAME = 'CS231n/assignment1/'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# Now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "\n",
        "# This downloads the CIFAR-10 dataset to your Drive\n",
        "# if it doesn't already exist.\n",
        "%cd drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
        "!bash get_datasets.sh\n",
        "%cd /content/drive/My\\ Drive/$FOLDERNAME"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/CS231n/assignment1/cs231n/datasets\n",
            "/content/drive/My Drive/CS231n/assignment1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-title"
        ],
        "id": "JE75Mztpvmtn"
      },
      "source": [
        "# Softmax exercise\n",
        "\n",
        "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
        "\n",
        "This exercise is analogous to the SVM exercise. You will:\n",
        "\n",
        "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
        "- implement the fully-vectorized expression for its **analytic gradient**\n",
        "- **check your implementation** with numerical gradient\n",
        "- use a validation set to **tune the learning rate and regularization** strength\n",
        "- **optimize** the loss function with **SGD**\n",
        "- **visualize** the final learned weights\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "8dLaJUk2vmto"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from cs231n.data_utils import load_CIFAR10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading extenrnal modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo-OYBDHvmto",
        "outputId": "539d5f14-46f3-490d-8644-b843ba3baf3c"
      },
      "source": [
        "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for the linear classifier. These are the same steps as we used for the\n",
        "    SVM, but condensed to a single function.  \n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
        "    \n",
        "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
        "    try:\n",
        "       del X_train, y_train\n",
        "       del X_test, y_test\n",
        "       print('Clear previously loaded data.')\n",
        "    except:\n",
        "       pass\n",
        "\n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "    \n",
        "    # subsample the data\n",
        "    mask = list(range(num_training, num_training + num_validation))\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = list(range(num_training))\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = list(range(num_test))\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
        "    X_dev = X_train[mask]\n",
        "    y_dev = y_train[mask]\n",
        "    \n",
        "    # Preprocessing: reshape the image data into rows\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
        "    \n",
        "    # Normalize the data: subtract the mean image\n",
        "    mean_image = np.mean(X_train, axis = 0)\n",
        "    X_train -= mean_image\n",
        "    X_val -= mean_image\n",
        "    X_test -= mean_image\n",
        "    X_dev -= mean_image\n",
        "    \n",
        "    # add bias dimension and transform into columns\n",
        "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
        "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
        "    \n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
        "\n",
        "\n",
        "# Invoke the above function to get our data.\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)\n",
        "print('dev data shape: ', X_dev.shape)\n",
        "print('dev labels shape: ', y_dev.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data shape:  (49000, 3073)\n",
            "Train labels shape:  (49000,)\n",
            "Validation data shape:  (1000, 3073)\n",
            "Validation labels shape:  (1000,)\n",
            "Test data shape:  (1000, 3073)\n",
            "Test labels shape:  (1000,)\n",
            "dev data shape:  (500, 3073)\n",
            "dev labels shape:  (500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEMHGdssvmtp"
      },
      "source": [
        "## Softmax Classifier\n",
        "\n",
        "Your code for this section will all be written inside `cs231n/classifiers/softmax.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuyhsQFFvmtq",
        "outputId": "888e7693-2b8b-46c3-b579-8b0cf6e82bb8"
      },
      "source": [
        "# First implement the naive softmax loss function with nested loops.\n",
        "# Open the file cs231n/classifiers/softmax.py and implement the\n",
        "# softmax_loss_naive function.\n",
        "\n",
        "from cs231n.classifiers.softmax import softmax_loss_naive\n",
        "import time\n",
        "\n",
        "# Generate a random softmax weight matrix and use it to compute the loss.\n",
        "W = np.random.randn(3073, 10) * 0.0001\n",
        "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
        "\n",
        "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
        "print('loss: %f' % loss)\n",
        "print('sanity check: %f' % (-np.log(0.1)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 2.396775\n",
            "sanity check: 2.302585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-inline"
        ],
        "id": "Jx0LCknSvmtq"
      },
      "source": [
        "**Inline Question 1**\n",
        "\n",
        "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
        "\n",
        "$\\color{blue}{\\textit Your Answer:}$ *Initialized W ~= 0, so L ~= sum(log(num_class)) / N = log(num_class) = log(10)  = -log(0.1)* \n",
        "*In other words, initialized probability of classifier is 0.1, so L ~= 0.1\n",
        "(softmax is the probability of class)*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqB2kvjVvmtr",
        "outputId": "2c02c452-51a4-4673-99b6-29eaff404f6f"
      },
      "source": [
        "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
        "# version of the gradient that uses nested loops.\n",
        "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
        "\n",
        "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
        "# The numeric gradient should be close to the analytic gradient.\n",
        "from cs231n.gradient_check import grad_check_sparse\n",
        "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
        "\n",
        "# similar to SVM case, do another gradient check with regularization\n",
        "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
        "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, grad, 10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numerical: 0.958637 analytic: 0.958637, relative error: 8.062512e-08\n",
            "numerical: 2.053204 analytic: 2.053204, relative error: 5.631668e-09\n",
            "numerical: -0.100689 analytic: -0.100689, relative error: 2.076191e-07\n",
            "numerical: -0.045662 analytic: -0.045662, relative error: 8.976642e-07\n",
            "numerical: 0.419911 analytic: 0.419911, relative error: 4.623502e-08\n",
            "numerical: -0.387887 analytic: -0.387887, relative error: 1.152877e-07\n",
            "numerical: 0.032316 analytic: 0.032316, relative error: 2.562168e-07\n",
            "numerical: -0.133469 analytic: -0.133469, relative error: 3.194916e-07\n",
            "numerical: 2.196909 analytic: 2.196909, relative error: 3.721690e-08\n",
            "numerical: 0.750982 analytic: 0.750982, relative error: 1.060935e-07\n",
            "numerical: -0.456776 analytic: -0.456776, relative error: 7.420475e-08\n",
            "numerical: -4.633023 analytic: -4.633023, relative error: 1.677925e-09\n",
            "numerical: 0.521908 analytic: 0.521908, relative error: 9.814182e-08\n",
            "numerical: -2.927417 analytic: -2.927417, relative error: 1.614907e-08\n",
            "numerical: 2.519272 analytic: 2.519272, relative error: 9.904942e-09\n",
            "numerical: 0.616725 analytic: 0.616725, relative error: 3.510593e-08\n",
            "numerical: -0.686828 analytic: -0.686828, relative error: 9.716424e-08\n",
            "numerical: 2.507295 analytic: 2.507295, relative error: 1.713590e-08\n",
            "numerical: 1.186538 analytic: 1.186538, relative error: 1.372281e-08\n",
            "numerical: -0.572647 analytic: -0.572647, relative error: 5.094814e-09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lto1zG4J8vd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a8f2ac-ceed-4071-bcf9-833cbf1aa828"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISiDKtBTbc2_"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sy_-MtSvmtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a49d234-9085-4331-acca-986dd3904bef"
      },
      "source": [
        "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
        "# implement a vectorized version in softmax_loss_vectorized.\n",
        "# The two versions should compute the same results, but the vectorized version should be\n",
        "# much faster.\n",
        "tic = time.time()\n",
        "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
        "\n",
        "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
        "tic = time.time()\n",
        "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
        "\n",
        "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
        "# of the gradient.\n",
        "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
        "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
        "print('Gradient difference: %f' % grad_difference)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "naive loss: 2.396775e+00 computed in 0.165621s\n",
            "vectorized loss: 2.396775e+00 computed in 0.018278s\n",
            "Loss difference: 0.000000\n",
            "Gradient difference: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuning",
        "tags": [
          "code"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff543091-58a0-4ad3-97a7-daf17a8c19fb"
      },
      "source": [
        "# Use the validation set to tune hyperparameters (regularization strength and\n",
        "# learning rate). You should experiment with different ranges for the learning\n",
        "# rates and regularization strengths; if you are careful you should be able to\n",
        "# get a classification accuracy of over 0.35 on the validation set.\n",
        "\n",
        "from cs231n.classifiers import Softmax\n",
        "results = {}\n",
        "best_val = -1\n",
        "best_softmax = None\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Use the validation set to set the learning rate and regularization strength. #\n",
        "# This should be identical to the validation that you did for the SVM; save    #\n",
        "# the best trained softmax classifer in best_softmax.                          #\n",
        "################################################################################\n",
        "\n",
        "# Provided as a reference. You may or may not want to change these hyperparameters\n",
        "learning_rates = np.arange(1e-7, 5e-5, 5e-6)\n",
        "regularization_strengths = np.arange(2.5e3, 5e4, 4.75e3)\n",
        "\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for rs in regularization_strengths:\n",
        "        softmax = Softmax()\n",
        "        loss_hist = softmax.train(X_train, y_train, learning_rate=lr, reg=rs,\n",
        "                      num_iters=1500, verbose=True)\n",
        "        \n",
        "        y_train_pred = softmax.predict(X_train)\n",
        "        y_val_pred = softmax.predict(X_val)\n",
        "        train_accuracy = np.mean(y_train == y_train_pred)\n",
        "        val_accuracy = np.mean(y_val == y_val_pred)\n",
        "        if best_val < val_accuracy:\n",
        "            best_val = val_accuracy\n",
        "            best_softmax = softmax\n",
        "        \n",
        "        results[(lr, rs)] = (train_accuracy, val_accuracy)\n",
        "    \n",
        "# Print out results.\n",
        "for lr, reg in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
        "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
        "                lr, reg, train_accuracy, val_accuracy))\n",
        "    \n",
        "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 / 1500: loss 82.511813\n",
            "iteration 100 / 1500: loss 73.382993\n",
            "iteration 200 / 1500: loss 66.026889\n",
            "iteration 300 / 1500: loss 59.755847\n",
            "iteration 400 / 1500: loss 53.994887\n",
            "iteration 500 / 1500: loss 48.958003\n",
            "iteration 600 / 1500: loss 44.191025\n",
            "iteration 700 / 1500: loss 40.237707\n",
            "iteration 800 / 1500: loss 36.442753\n",
            "iteration 900 / 1500: loss 33.146157\n",
            "iteration 1000 / 1500: loss 30.228066\n",
            "iteration 1100 / 1500: loss 27.361720\n",
            "iteration 1200 / 1500: loss 24.903573\n",
            "iteration 1300 / 1500: loss 22.762789\n",
            "iteration 1400 / 1500: loss 20.677516\n",
            "iteration 0 / 1500: loss 226.387604\n",
            "iteration 100 / 1500: loss 168.921298\n",
            "iteration 200 / 1500: loss 126.501303\n",
            "iteration 300 / 1500: loss 94.606075\n",
            "iteration 400 / 1500: loss 71.178172\n",
            "iteration 500 / 1500: loss 53.577697\n",
            "iteration 600 / 1500: loss 40.554052\n",
            "iteration 700 / 1500: loss 30.771094\n",
            "iteration 800 / 1500: loss 23.435852\n",
            "iteration 900 / 1500: loss 17.923924\n",
            "iteration 1000 / 1500: loss 13.978230\n",
            "iteration 1100 / 1500: loss 10.964361\n",
            "iteration 1200 / 1500: loss 8.704377\n",
            "iteration 1300 / 1500: loss 6.905592\n",
            "iteration 1400 / 1500: loss 5.715320\n",
            "iteration 0 / 1500: loss 369.181837\n",
            "iteration 100 / 1500: loss 227.330721\n",
            "iteration 200 / 1500: loss 140.774553\n",
            "iteration 300 / 1500: loss 87.778623\n",
            "iteration 400 / 1500: loss 54.838372\n",
            "iteration 500 / 1500: loss 34.642141\n",
            "iteration 600 / 1500: loss 22.191151\n",
            "iteration 700 / 1500: loss 14.416783\n",
            "iteration 800 / 1500: loss 9.668381\n",
            "iteration 900 / 1500: loss 6.793767\n",
            "iteration 1000 / 1500: loss 4.939813\n",
            "iteration 1100 / 1500: loss 3.873495\n",
            "iteration 1200 / 1500: loss 3.122214\n",
            "iteration 1300 / 1500: loss 2.711685\n",
            "iteration 1400 / 1500: loss 2.467370\n",
            "iteration 0 / 1500: loss 526.371742\n",
            "iteration 100 / 1500: loss 268.718817\n",
            "iteration 200 / 1500: loss 138.073708\n",
            "iteration 300 / 1500: loss 71.441493\n",
            "iteration 400 / 1500: loss 37.423570\n",
            "iteration 500 / 1500: loss 20.070900\n",
            "iteration 600 / 1500: loss 11.244925\n",
            "iteration 700 / 1500: loss 6.743007\n",
            "iteration 800 / 1500: loss 4.388007\n",
            "iteration 900 / 1500: loss 3.364196\n",
            "iteration 1000 / 1500: loss 2.634477\n",
            "iteration 1100 / 1500: loss 2.372335\n",
            "iteration 1200 / 1500: loss 2.257142\n",
            "iteration 1300 / 1500: loss 2.108586\n",
            "iteration 1400 / 1500: loss 2.071526\n",
            "iteration 0 / 1500: loss 672.085439\n",
            "iteration 100 / 1500: loss 283.549784\n",
            "iteration 200 / 1500: loss 120.613324\n",
            "iteration 300 / 1500: loss 52.066948\n",
            "iteration 400 / 1500: loss 23.093162\n",
            "iteration 500 / 1500: loss 10.944338\n",
            "iteration 600 / 1500: loss 5.833216\n",
            "iteration 700 / 1500: loss 3.568944\n",
            "iteration 800 / 1500: loss 2.781788\n",
            "iteration 900 / 1500: loss 2.317110\n",
            "iteration 1000 / 1500: loss 2.230430\n",
            "iteration 1100 / 1500: loss 2.107441\n",
            "iteration 1200 / 1500: loss 2.063054\n",
            "iteration 1300 / 1500: loss 2.090675\n",
            "iteration 1400 / 1500: loss 2.104082\n",
            "iteration 0 / 1500: loss 806.461216\n",
            "iteration 100 / 1500: loss 281.708321\n",
            "iteration 200 / 1500: loss 99.380751\n",
            "iteration 300 / 1500: loss 35.922036\n",
            "iteration 400 / 1500: loss 13.879135\n",
            "iteration 500 / 1500: loss 6.245875\n",
            "iteration 600 / 1500: loss 3.445898\n",
            "iteration 700 / 1500: loss 2.500848\n",
            "iteration 800 / 1500: loss 2.246214\n",
            "iteration 900 / 1500: loss 2.178204\n",
            "iteration 1000 / 1500: loss 2.128630\n",
            "iteration 1100 / 1500: loss 2.090430\n",
            "iteration 1200 / 1500: loss 2.092938\n",
            "iteration 1300 / 1500: loss 2.120433\n",
            "iteration 1400 / 1500: loss 2.097081\n",
            "iteration 0 / 1500: loss 970.390446\n",
            "iteration 100 / 1500: loss 279.998498\n",
            "iteration 200 / 1500: loss 81.901632\n",
            "iteration 300 / 1500: loss 25.054066\n",
            "iteration 400 / 1500: loss 8.631320\n",
            "iteration 500 / 1500: loss 3.980033\n",
            "iteration 600 / 1500: loss 2.641656\n",
            "iteration 700 / 1500: loss 2.203851\n",
            "iteration 800 / 1500: loss 2.146862\n",
            "iteration 900 / 1500: loss 2.108528\n",
            "iteration 1000 / 1500: loss 2.156878\n",
            "iteration 1100 / 1500: loss 2.104662\n",
            "iteration 1200 / 1500: loss 2.078464\n",
            "iteration 1300 / 1500: loss 2.127601\n",
            "iteration 1400 / 1500: loss 2.072101\n",
            "iteration 0 / 1500: loss 1106.709359\n",
            "iteration 100 / 1500: loss 264.137643\n",
            "iteration 200 / 1500: loss 64.296568\n",
            "iteration 300 / 1500: loss 16.880020\n",
            "iteration 400 / 1500: loss 5.611161\n",
            "iteration 500 / 1500: loss 2.939327\n",
            "iteration 600 / 1500: loss 2.352991\n",
            "iteration 700 / 1500: loss 2.093926\n",
            "iteration 800 / 1500: loss 2.124343\n",
            "iteration 900 / 1500: loss 2.139450\n",
            "iteration 1000 / 1500: loss 2.093317\n",
            "iteration 1100 / 1500: loss 2.123573\n",
            "iteration 1200 / 1500: loss 2.076180\n",
            "iteration 1300 / 1500: loss 2.090600\n",
            "iteration 1400 / 1500: loss 2.141981\n",
            "iteration 0 / 1500: loss 1264.195647\n",
            "iteration 100 / 1500: loss 249.025938\n",
            "iteration 200 / 1500: loss 50.447146\n",
            "iteration 300 / 1500: loss 11.626604\n",
            "iteration 400 / 1500: loss 4.000804\n",
            "iteration 500 / 1500: loss 2.451888\n",
            "iteration 600 / 1500: loss 2.236336\n",
            "iteration 700 / 1500: loss 2.113364\n",
            "iteration 800 / 1500: loss 2.102225\n",
            "iteration 900 / 1500: loss 2.105005\n",
            "iteration 1000 / 1500: loss 2.121973\n",
            "iteration 1100 / 1500: loss 2.137274\n",
            "iteration 1200 / 1500: loss 2.185700\n",
            "iteration 1300 / 1500: loss 2.156792\n",
            "iteration 1400 / 1500: loss 2.146500\n",
            "iteration 0 / 1500: loss 1405.748306\n",
            "iteration 100 / 1500: loss 228.886129\n",
            "iteration 200 / 1500: loss 38.793653\n",
            "iteration 300 / 1500: loss 8.063170\n",
            "iteration 400 / 1500: loss 3.077478\n",
            "iteration 500 / 1500: loss 2.285459\n",
            "iteration 600 / 1500: loss 2.150705\n",
            "iteration 700 / 1500: loss 2.132983\n",
            "iteration 800 / 1500: loss 2.109273\n",
            "iteration 900 / 1500: loss 2.138536\n",
            "iteration 1000 / 1500: loss 2.114190\n",
            "iteration 1100 / 1500: loss 2.101036\n",
            "iteration 1200 / 1500: loss 2.222022\n",
            "iteration 1300 / 1500: loss 2.121884\n",
            "iteration 1400 / 1500: loss 2.156968\n",
            "iteration 0 / 1500: loss 82.146306\n",
            "iteration 100 / 1500: loss 2.360897\n",
            "iteration 200 / 1500: loss 2.131208\n",
            "iteration 300 / 1500: loss 2.086273\n",
            "iteration 400 / 1500: loss 2.004961\n",
            "iteration 500 / 1500: loss 2.118610\n",
            "iteration 600 / 1500: loss 2.032328\n",
            "iteration 700 / 1500: loss 2.101499\n",
            "iteration 800 / 1500: loss 2.226196\n",
            "iteration 900 / 1500: loss 1.997722\n",
            "iteration 1000 / 1500: loss 2.130536\n",
            "iteration 1100 / 1500: loss 1.981241\n",
            "iteration 1200 / 1500: loss 1.988269\n",
            "iteration 1300 / 1500: loss 2.019473\n",
            "iteration 1400 / 1500: loss 2.182434\n",
            "iteration 0 / 1500: loss 227.965799\n",
            "iteration 100 / 1500: loss 2.104599\n",
            "iteration 200 / 1500: loss 2.475040\n",
            "iteration 300 / 1500: loss 2.060465\n",
            "iteration 400 / 1500: loss 2.103927\n",
            "iteration 500 / 1500: loss 2.116924\n",
            "iteration 600 / 1500: loss 2.170384\n",
            "iteration 700 / 1500: loss 2.415115\n",
            "iteration 800 / 1500: loss 2.249296\n",
            "iteration 900 / 1500: loss 2.388656\n",
            "iteration 1000 / 1500: loss 2.188889\n",
            "iteration 1100 / 1500: loss 2.265948\n",
            "iteration 1200 / 1500: loss 2.274219\n",
            "iteration 1300 / 1500: loss 2.290708\n",
            "iteration 1400 / 1500: loss 2.279946\n",
            "iteration 0 / 1500: loss 373.928826\n",
            "iteration 100 / 1500: loss 2.382410\n",
            "iteration 200 / 1500: loss 2.243965\n",
            "iteration 300 / 1500: loss 2.352319\n",
            "iteration 400 / 1500: loss 3.108852\n",
            "iteration 500 / 1500: loss 2.293391\n",
            "iteration 600 / 1500: loss 2.369008\n",
            "iteration 700 / 1500: loss 2.334744\n",
            "iteration 800 / 1500: loss 2.137753\n",
            "iteration 900 / 1500: loss 2.150973\n",
            "iteration 1000 / 1500: loss 2.276977\n",
            "iteration 1100 / 1500: loss 2.201019\n",
            "iteration 1200 / 1500: loss 2.168459\n",
            "iteration 1300 / 1500: loss 2.315997\n",
            "iteration 1400 / 1500: loss 2.187260\n",
            "iteration 0 / 1500: loss 517.484826\n",
            "iteration 100 / 1500: loss 2.640546\n",
            "iteration 200 / 1500: loss 2.573486\n",
            "iteration 300 / 1500: loss 2.382490\n",
            "iteration 400 / 1500: loss 2.294277\n",
            "iteration 500 / 1500: loss 2.122395\n",
            "iteration 600 / 1500: loss 2.296191\n",
            "iteration 700 / 1500: loss 2.547659\n",
            "iteration 800 / 1500: loss 3.064772\n",
            "iteration 900 / 1500: loss 2.420911\n",
            "iteration 1000 / 1500: loss 2.560651\n",
            "iteration 1100 / 1500: loss 2.273079\n",
            "iteration 1200 / 1500: loss 2.181383\n",
            "iteration 1300 / 1500: loss 2.249725\n",
            "iteration 1400 / 1500: loss 2.684294\n",
            "iteration 0 / 1500: loss 666.003280\n",
            "iteration 100 / 1500: loss 2.505244\n",
            "iteration 200 / 1500: loss 2.337783\n",
            "iteration 300 / 1500: loss 2.566198\n",
            "iteration 400 / 1500: loss 2.763523\n",
            "iteration 500 / 1500: loss 2.451641\n",
            "iteration 600 / 1500: loss 2.551492\n",
            "iteration 700 / 1500: loss 2.905720\n",
            "iteration 800 / 1500: loss 2.505891\n",
            "iteration 900 / 1500: loss 3.046562\n",
            "iteration 1000 / 1500: loss 2.618772\n",
            "iteration 1100 / 1500: loss 2.194858\n",
            "iteration 1200 / 1500: loss 2.544347\n",
            "iteration 1300 / 1500: loss 2.265510\n",
            "iteration 1400 / 1500: loss 2.605693\n",
            "iteration 0 / 1500: loss 806.272013\n",
            "iteration 100 / 1500: loss 2.677039\n",
            "iteration 200 / 1500: loss 2.564209\n",
            "iteration 300 / 1500: loss 2.732380\n",
            "iteration 400 / 1500: loss 2.492091\n",
            "iteration 500 / 1500: loss 3.106730\n",
            "iteration 600 / 1500: loss 2.607315\n",
            "iteration 700 / 1500: loss 3.381050\n",
            "iteration 800 / 1500: loss 2.559068\n",
            "iteration 900 / 1500: loss 2.425731\n",
            "iteration 1000 / 1500: loss 2.880460\n",
            "iteration 1100 / 1500: loss 3.285578\n",
            "iteration 1200 / 1500: loss 2.622500\n",
            "iteration 1300 / 1500: loss 2.548842\n",
            "iteration 1400 / 1500: loss 2.333362\n",
            "iteration 0 / 1500: loss 948.884232\n",
            "iteration 100 / 1500: loss 2.887468\n",
            "iteration 200 / 1500: loss 2.604911\n",
            "iteration 300 / 1500: loss 2.447221\n",
            "iteration 400 / 1500: loss 3.899078\n",
            "iteration 500 / 1500: loss 3.009623\n",
            "iteration 600 / 1500: loss 2.328964\n",
            "iteration 700 / 1500: loss 2.730440\n",
            "iteration 800 / 1500: loss 2.957033\n",
            "iteration 900 / 1500: loss 2.341753\n",
            "iteration 1000 / 1500: loss 3.246622\n",
            "iteration 1100 / 1500: loss 2.900587\n",
            "iteration 1200 / 1500: loss 2.977765\n",
            "iteration 1300 / 1500: loss 2.673673\n",
            "iteration 1400 / 1500: loss 2.888645\n",
            "iteration 0 / 1500: loss 1105.393226\n",
            "iteration 100 / 1500: loss 3.164462\n",
            "iteration 200 / 1500: loss 2.966655\n",
            "iteration 300 / 1500: loss 2.648805\n",
            "iteration 400 / 1500: loss 2.663190\n",
            "iteration 500 / 1500: loss 2.666174\n",
            "iteration 600 / 1500: loss 2.645113\n",
            "iteration 700 / 1500: loss 2.680518\n",
            "iteration 800 / 1500: loss 2.478116\n",
            "iteration 900 / 1500: loss 3.889354\n",
            "iteration 1000 / 1500: loss 3.162525\n",
            "iteration 1100 / 1500: loss 3.305117\n",
            "iteration 1200 / 1500: loss 3.312044\n",
            "iteration 1300 / 1500: loss 2.573285\n",
            "iteration 1400 / 1500: loss 2.642879\n",
            "iteration 0 / 1500: loss 1265.789407\n",
            "iteration 100 / 1500: loss 3.801647\n",
            "iteration 200 / 1500: loss 3.538294\n",
            "iteration 300 / 1500: loss 3.288221\n",
            "iteration 400 / 1500: loss 3.085258\n",
            "iteration 500 / 1500: loss 3.364009\n",
            "iteration 600 / 1500: loss 3.186289\n",
            "iteration 700 / 1500: loss 3.467544\n",
            "iteration 800 / 1500: loss 3.630009\n",
            "iteration 900 / 1500: loss 3.053360\n",
            "iteration 1000 / 1500: loss 2.621475\n",
            "iteration 1100 / 1500: loss 3.654005\n",
            "iteration 1200 / 1500: loss 3.803920\n",
            "iteration 1300 / 1500: loss 2.521760\n",
            "iteration 1400 / 1500: loss 3.186469\n",
            "iteration 0 / 1500: loss 1421.806705\n",
            "iteration 100 / 1500: loss 3.937080\n",
            "iteration 200 / 1500: loss 3.153387\n",
            "iteration 300 / 1500: loss 2.815397\n",
            "iteration 400 / 1500: loss 3.548039\n",
            "iteration 500 / 1500: loss 3.066486\n",
            "iteration 600 / 1500: loss 2.609766\n",
            "iteration 700 / 1500: loss 3.536308\n",
            "iteration 800 / 1500: loss 3.501381\n",
            "iteration 900 / 1500: loss 2.963503\n",
            "iteration 1000 / 1500: loss 4.318377\n",
            "iteration 1100 / 1500: loss 3.884400\n",
            "iteration 1200 / 1500: loss 3.219086\n",
            "iteration 1300 / 1500: loss 3.261377\n",
            "iteration 1400 / 1500: loss 3.246466\n",
            "iteration 0 / 1500: loss 81.799739\n",
            "iteration 100 / 1500: loss 3.252578\n",
            "iteration 200 / 1500: loss 4.257987\n",
            "iteration 300 / 1500: loss 3.613815\n",
            "iteration 400 / 1500: loss 3.796892\n",
            "iteration 500 / 1500: loss 3.985449\n",
            "iteration 600 / 1500: loss 4.551120\n",
            "iteration 700 / 1500: loss 4.296699\n",
            "iteration 800 / 1500: loss 4.438368\n",
            "iteration 900 / 1500: loss 2.904599\n",
            "iteration 1000 / 1500: loss 3.919927\n",
            "iteration 1100 / 1500: loss 6.036981\n",
            "iteration 1200 / 1500: loss 3.529733\n",
            "iteration 1300 / 1500: loss 2.675462\n",
            "iteration 1400 / 1500: loss 3.465779\n",
            "iteration 0 / 1500: loss 226.417020\n",
            "iteration 100 / 1500: loss 4.913289\n",
            "iteration 200 / 1500: loss 4.943025\n",
            "iteration 300 / 1500: loss 4.692394\n",
            "iteration 400 / 1500: loss 6.037043\n",
            "iteration 500 / 1500: loss 3.894475\n",
            "iteration 600 / 1500: loss 3.101444\n",
            "iteration 700 / 1500: loss 3.947590\n",
            "iteration 800 / 1500: loss 4.368628\n",
            "iteration 900 / 1500: loss 6.464057\n",
            "iteration 1000 / 1500: loss 4.592100\n",
            "iteration 1100 / 1500: loss 3.847925\n",
            "iteration 1200 / 1500: loss 5.052584\n",
            "iteration 1300 / 1500: loss 4.203955\n",
            "iteration 1400 / 1500: loss 4.591963\n",
            "iteration 0 / 1500: loss 375.085619\n",
            "iteration 100 / 1500: loss 5.820720\n",
            "iteration 200 / 1500: loss 5.511084\n",
            "iteration 300 / 1500: loss 4.108948\n",
            "iteration 400 / 1500: loss 4.309010\n",
            "iteration 500 / 1500: loss 5.246219\n",
            "iteration 600 / 1500: loss 5.132935\n",
            "iteration 700 / 1500: loss 5.056089\n",
            "iteration 800 / 1500: loss 5.261128\n",
            "iteration 900 / 1500: loss 4.836227\n",
            "iteration 1000 / 1500: loss 4.028466\n",
            "iteration 1100 / 1500: loss 8.277032\n",
            "iteration 1200 / 1500: loss 4.992682\n",
            "iteration 1300 / 1500: loss 5.593053\n",
            "iteration 1400 / 1500: loss 5.427848\n",
            "iteration 0 / 1500: loss 518.783333\n",
            "iteration 100 / 1500: loss 6.381076\n",
            "iteration 200 / 1500: loss 7.219381\n",
            "iteration 300 / 1500: loss 5.736201\n",
            "iteration 400 / 1500: loss 4.667635\n",
            "iteration 500 / 1500: loss 7.411600\n",
            "iteration 600 / 1500: loss 5.426150\n",
            "iteration 700 / 1500: loss 3.580353\n",
            "iteration 800 / 1500: loss 6.487119\n",
            "iteration 900 / 1500: loss 8.371802\n",
            "iteration 1000 / 1500: loss 7.030326\n",
            "iteration 1100 / 1500: loss 8.548206\n",
            "iteration 1200 / 1500: loss 5.964387\n",
            "iteration 1300 / 1500: loss 5.434137\n",
            "iteration 1400 / 1500: loss 7.267578\n",
            "iteration 0 / 1500: loss 673.867800\n",
            "iteration 100 / 1500: loss 7.911981\n",
            "iteration 200 / 1500: loss 8.042653\n",
            "iteration 300 / 1500: loss 8.896059\n",
            "iteration 400 / 1500: loss 5.766984\n",
            "iteration 500 / 1500: loss 6.137266\n",
            "iteration 600 / 1500: loss 7.826171\n",
            "iteration 700 / 1500: loss 5.348434\n",
            "iteration 800 / 1500: loss 6.104875\n",
            "iteration 900 / 1500: loss 7.952067\n",
            "iteration 1000 / 1500: loss 9.741130\n",
            "iteration 1100 / 1500: loss 7.569536\n",
            "iteration 1200 / 1500: loss 7.066768\n",
            "iteration 1300 / 1500: loss 5.882301\n",
            "iteration 1400 / 1500: loss 7.010911\n",
            "iteration 0 / 1500: loss 816.725554\n",
            "iteration 100 / 1500: loss 6.174811\n",
            "iteration 200 / 1500: loss 6.960067\n",
            "iteration 300 / 1500: loss 6.922645\n",
            "iteration 400 / 1500: loss 4.833558\n",
            "iteration 500 / 1500: loss 8.013679\n",
            "iteration 600 / 1500: loss 9.372272\n",
            "iteration 700 / 1500: loss 9.143609\n",
            "iteration 800 / 1500: loss 7.150203\n",
            "iteration 900 / 1500: loss 9.079929\n",
            "iteration 1000 / 1500: loss 8.934378\n",
            "iteration 1100 / 1500: loss 7.559971\n",
            "iteration 1200 / 1500: loss 8.880784\n",
            "iteration 1300 / 1500: loss 9.749764\n",
            "iteration 1400 / 1500: loss 7.322821\n",
            "iteration 0 / 1500: loss 965.276584\n",
            "iteration 100 / 1500: loss 7.886210\n",
            "iteration 200 / 1500: loss 8.285112\n",
            "iteration 300 / 1500: loss 11.175889\n",
            "iteration 400 / 1500: loss 9.995637\n",
            "iteration 500 / 1500: loss 10.967718\n",
            "iteration 600 / 1500: loss 12.052509\n",
            "iteration 700 / 1500: loss 10.864407\n",
            "iteration 800 / 1500: loss 10.840360\n",
            "iteration 900 / 1500: loss 9.780925\n",
            "iteration 1000 / 1500: loss 9.189966\n",
            "iteration 1100 / 1500: loss 9.794514\n",
            "iteration 1200 / 1500: loss 10.769847\n",
            "iteration 1300 / 1500: loss 6.755775\n",
            "iteration 1400 / 1500: loss 8.226332\n",
            "iteration 0 / 1500: loss 1091.434112\n",
            "iteration 100 / 1500: loss 9.329664\n",
            "iteration 200 / 1500: loss 9.174143\n",
            "iteration 300 / 1500: loss 10.329435\n",
            "iteration 400 / 1500: loss 9.417096\n",
            "iteration 500 / 1500: loss 9.328120\n",
            "iteration 600 / 1500: loss 10.819012\n",
            "iteration 700 / 1500: loss 9.161968\n",
            "iteration 800 / 1500: loss 9.480167\n",
            "iteration 900 / 1500: loss 9.907952\n",
            "iteration 1000 / 1500: loss 8.960765\n",
            "iteration 1100 / 1500: loss 8.931317\n",
            "iteration 1200 / 1500: loss 10.668926\n",
            "iteration 1300 / 1500: loss 6.833568\n",
            "iteration 1400 / 1500: loss 8.858149\n",
            "iteration 0 / 1500: loss 1256.225022\n",
            "iteration 100 / 1500: loss 10.937604\n",
            "iteration 200 / 1500: loss 10.435528\n",
            "iteration 300 / 1500: loss 11.036568\n",
            "iteration 400 / 1500: loss 12.756555\n",
            "iteration 500 / 1500: loss 10.946282\n",
            "iteration 600 / 1500: loss 10.795829\n",
            "iteration 700 / 1500: loss 10.043643\n",
            "iteration 800 / 1500: loss 9.065756\n",
            "iteration 900 / 1500: loss 9.998308\n",
            "iteration 1000 / 1500: loss 10.415196\n",
            "iteration 1100 / 1500: loss 12.315266\n",
            "iteration 1200 / 1500: loss 10.183905\n",
            "iteration 1300 / 1500: loss 12.073402\n",
            "iteration 1400 / 1500: loss 11.329947\n",
            "iteration 0 / 1500: loss 1424.884036\n",
            "iteration 100 / 1500: loss 13.643277\n",
            "iteration 200 / 1500: loss 12.619532\n",
            "iteration 300 / 1500: loss 12.229040\n",
            "iteration 400 / 1500: loss 12.412637\n",
            "iteration 500 / 1500: loss 11.691504\n",
            "iteration 600 / 1500: loss 11.768917\n",
            "iteration 700 / 1500: loss 12.132413\n",
            "iteration 800 / 1500: loss 15.556222\n",
            "iteration 900 / 1500: loss 12.658208\n",
            "iteration 1000 / 1500: loss 14.539812\n",
            "iteration 1100 / 1500: loss 11.620730\n",
            "iteration 1200 / 1500: loss 12.950614\n",
            "iteration 1300 / 1500: loss 14.152586\n",
            "iteration 1400 / 1500: loss 10.797659\n",
            "iteration 0 / 1500: loss 82.991722\n",
            "iteration 100 / 1500: loss 6.038317\n",
            "iteration 200 / 1500: loss 4.801840\n",
            "iteration 300 / 1500: loss 5.470574\n",
            "iteration 400 / 1500: loss 4.044195\n",
            "iteration 500 / 1500: loss 8.443632\n",
            "iteration 600 / 1500: loss 5.959389\n",
            "iteration 700 / 1500: loss 6.625126\n",
            "iteration 800 / 1500: loss 6.588235\n",
            "iteration 900 / 1500: loss 5.613027\n",
            "iteration 1000 / 1500: loss 8.689086\n",
            "iteration 1100 / 1500: loss 5.467407\n",
            "iteration 1200 / 1500: loss 5.036658\n",
            "iteration 1300 / 1500: loss 5.244057\n",
            "iteration 1400 / 1500: loss 4.484484\n",
            "iteration 0 / 1500: loss 227.696544\n",
            "iteration 100 / 1500: loss 10.718288\n",
            "iteration 200 / 1500: loss 6.848324\n",
            "iteration 300 / 1500: loss 7.548748\n",
            "iteration 400 / 1500: loss 5.558497\n",
            "iteration 500 / 1500: loss 8.904003\n",
            "iteration 600 / 1500: loss 11.369166\n",
            "iteration 700 / 1500: loss 8.515371\n",
            "iteration 800 / 1500: loss 4.935514\n",
            "iteration 900 / 1500: loss 8.059926\n",
            "iteration 1000 / 1500: loss 10.624170\n",
            "iteration 1100 / 1500: loss 8.400083\n",
            "iteration 1200 / 1500: loss 10.614351\n",
            "iteration 1300 / 1500: loss 9.248630\n",
            "iteration 1400 / 1500: loss 6.084663\n",
            "iteration 0 / 1500: loss 374.862869\n",
            "iteration 100 / 1500: loss 9.469032\n",
            "iteration 200 / 1500: loss 7.592112\n",
            "iteration 300 / 1500: loss 10.807321\n",
            "iteration 400 / 1500: loss 7.336058\n",
            "iteration 500 / 1500: loss 10.815765\n",
            "iteration 600 / 1500: loss 13.681885\n",
            "iteration 700 / 1500: loss 7.454519\n",
            "iteration 800 / 1500: loss 10.035238\n",
            "iteration 900 / 1500: loss 9.892385\n",
            "iteration 1000 / 1500: loss 10.473339\n",
            "iteration 1100 / 1500: loss 12.395745\n",
            "iteration 1200 / 1500: loss 9.623525\n",
            "iteration 1300 / 1500: loss 11.556929\n",
            "iteration 1400 / 1500: loss 12.111193\n",
            "iteration 0 / 1500: loss 519.588913\n",
            "iteration 100 / 1500: loss 17.577544\n",
            "iteration 200 / 1500: loss 13.181565\n",
            "iteration 300 / 1500: loss 13.212267\n",
            "iteration 400 / 1500: loss 10.201434\n",
            "iteration 500 / 1500: loss 9.611572\n",
            "iteration 600 / 1500: loss 13.269858\n",
            "iteration 700 / 1500: loss 10.773783\n",
            "iteration 800 / 1500: loss 16.992860\n",
            "iteration 900 / 1500: loss 7.661561\n",
            "iteration 1000 / 1500: loss 11.053538\n",
            "iteration 1100 / 1500: loss 12.834234\n",
            "iteration 1200 / 1500: loss 14.869351\n",
            "iteration 1300 / 1500: loss 8.521789\n",
            "iteration 1400 / 1500: loss 9.175268\n",
            "iteration 0 / 1500: loss 666.416408\n",
            "iteration 100 / 1500: loss 15.937483\n",
            "iteration 200 / 1500: loss 16.273781\n",
            "iteration 300 / 1500: loss 13.758106\n",
            "iteration 400 / 1500: loss 13.003743\n",
            "iteration 500 / 1500: loss 13.981153\n",
            "iteration 600 / 1500: loss 14.933054\n",
            "iteration 700 / 1500: loss 14.804511\n",
            "iteration 800 / 1500: loss 15.584907\n",
            "iteration 900 / 1500: loss 15.211805\n",
            "iteration 1000 / 1500: loss 10.802913\n",
            "iteration 1100 / 1500: loss 13.464380\n",
            "iteration 1200 / 1500: loss 14.436704\n",
            "iteration 1300 / 1500: loss 12.947954\n",
            "iteration 1400 / 1500: loss 16.091764\n",
            "iteration 0 / 1500: loss 816.187726\n",
            "iteration 100 / 1500: loss 17.673180\n",
            "iteration 200 / 1500: loss 19.160968\n",
            "iteration 300 / 1500: loss 21.397267\n",
            "iteration 400 / 1500: loss 22.639728\n",
            "iteration 500 / 1500: loss 17.354437\n",
            "iteration 600 / 1500: loss 20.718827\n",
            "iteration 700 / 1500: loss 21.944961\n",
            "iteration 800 / 1500: loss 19.830681\n",
            "iteration 900 / 1500: loss 17.928367\n",
            "iteration 1000 / 1500: loss 20.592377\n",
            "iteration 1100 / 1500: loss 17.856674\n",
            "iteration 1200 / 1500: loss 17.974098\n",
            "iteration 1300 / 1500: loss 17.895286\n",
            "iteration 1400 / 1500: loss 16.333799\n",
            "iteration 0 / 1500: loss 948.285305\n",
            "iteration 100 / 1500: loss 22.884378\n",
            "iteration 200 / 1500: loss 27.431691\n",
            "iteration 300 / 1500: loss 19.886249\n",
            "iteration 400 / 1500: loss 29.525847\n",
            "iteration 500 / 1500: loss 29.752290\n",
            "iteration 600 / 1500: loss 20.857851\n",
            "iteration 700 / 1500: loss 23.527006\n",
            "iteration 800 / 1500: loss 22.821096\n",
            "iteration 900 / 1500: loss 23.069657\n",
            "iteration 1000 / 1500: loss 21.336194\n",
            "iteration 1100 / 1500: loss 25.886253\n",
            "iteration 1200 / 1500: loss 25.572040\n",
            "iteration 1300 / 1500: loss 25.529057\n",
            "iteration 1400 / 1500: loss 22.336807\n",
            "iteration 0 / 1500: loss 1094.630645\n",
            "iteration 100 / 1500: loss 27.065987\n",
            "iteration 200 / 1500: loss 28.858253\n",
            "iteration 300 / 1500: loss 34.287073\n",
            "iteration 400 / 1500: loss 33.401020\n",
            "iteration 500 / 1500: loss 28.339490\n",
            "iteration 600 / 1500: loss 31.754238\n",
            "iteration 700 / 1500: loss 29.817395\n",
            "iteration 800 / 1500: loss 27.177838\n",
            "iteration 900 / 1500: loss 30.466942\n",
            "iteration 1000 / 1500: loss 31.318914\n",
            "iteration 1100 / 1500: loss 36.422481\n",
            "iteration 1200 / 1500: loss 33.573107\n",
            "iteration 1300 / 1500: loss 29.076471\n",
            "iteration 1400 / 1500: loss 30.606488\n",
            "iteration 0 / 1500: loss 1247.568904\n",
            "iteration 100 / 1500: loss 42.102984\n",
            "iteration 200 / 1500: loss 40.739021\n",
            "iteration 300 / 1500: loss 47.012444\n",
            "iteration 400 / 1500: loss 44.401159\n",
            "iteration 500 / 1500: loss 35.563763\n",
            "iteration 600 / 1500: loss 42.761759\n",
            "iteration 700 / 1500: loss 40.211242\n",
            "iteration 800 / 1500: loss 48.580681\n",
            "iteration 900 / 1500: loss 42.369387\n",
            "iteration 1000 / 1500: loss 41.687459\n",
            "iteration 1100 / 1500: loss 40.404515\n",
            "iteration 1200 / 1500: loss 42.553341\n",
            "iteration 1300 / 1500: loss 36.677833\n",
            "iteration 1400 / 1500: loss 42.247583\n",
            "iteration 0 / 1500: loss 1399.898145\n",
            "iteration 100 / 1500: loss 59.675437\n",
            "iteration 200 / 1500: loss 56.000488\n",
            "iteration 300 / 1500: loss 49.895857\n",
            "iteration 400 / 1500: loss 53.046575\n",
            "iteration 500 / 1500: loss 55.833094\n",
            "iteration 600 / 1500: loss 55.548239\n",
            "iteration 700 / 1500: loss 57.331147\n",
            "iteration 800 / 1500: loss 55.743282\n",
            "iteration 900 / 1500: loss 57.618758\n",
            "iteration 1000 / 1500: loss 58.092330\n",
            "iteration 1100 / 1500: loss 47.979429\n",
            "iteration 1200 / 1500: loss 59.059730\n",
            "iteration 1300 / 1500: loss 56.977058\n",
            "iteration 1400 / 1500: loss 61.422752\n",
            "iteration 0 / 1500: loss 82.841476\n",
            "iteration 100 / 1500: loss 10.372169\n",
            "iteration 200 / 1500: loss 10.273259\n",
            "iteration 300 / 1500: loss 9.109202\n",
            "iteration 400 / 1500: loss 8.474184\n",
            "iteration 500 / 1500: loss 7.107988\n",
            "iteration 600 / 1500: loss 6.713199\n",
            "iteration 700 / 1500: loss 9.853863\n",
            "iteration 800 / 1500: loss 6.269484\n",
            "iteration 900 / 1500: loss 12.693497\n",
            "iteration 1000 / 1500: loss 5.654422\n",
            "iteration 1100 / 1500: loss 7.278951\n",
            "iteration 1200 / 1500: loss 7.353885\n",
            "iteration 1300 / 1500: loss 7.194409\n",
            "iteration 1400 / 1500: loss 8.122538\n",
            "iteration 0 / 1500: loss 227.823681\n",
            "iteration 100 / 1500: loss 10.519012\n",
            "iteration 200 / 1500: loss 14.546998\n",
            "iteration 300 / 1500: loss 11.102349\n",
            "iteration 400 / 1500: loss 8.759148\n",
            "iteration 500 / 1500: loss 12.756303\n",
            "iteration 600 / 1500: loss 12.569977\n",
            "iteration 700 / 1500: loss 10.604118\n",
            "iteration 800 / 1500: loss 10.850535\n",
            "iteration 900 / 1500: loss 12.002076\n",
            "iteration 1000 / 1500: loss 13.630774\n",
            "iteration 1100 / 1500: loss 9.507642\n",
            "iteration 1200 / 1500: loss 9.339521\n",
            "iteration 1300 / 1500: loss 7.351303\n",
            "iteration 1400 / 1500: loss 14.465148\n",
            "iteration 0 / 1500: loss 370.169840\n",
            "iteration 100 / 1500: loss 23.929593\n",
            "iteration 200 / 1500: loss 14.982680\n",
            "iteration 300 / 1500: loss 14.050419\n",
            "iteration 400 / 1500: loss 19.855867\n",
            "iteration 500 / 1500: loss 16.848423\n",
            "iteration 600 / 1500: loss 16.455937\n",
            "iteration 700 / 1500: loss 16.423253\n",
            "iteration 800 / 1500: loss 10.554887\n",
            "iteration 900 / 1500: loss 9.635638\n",
            "iteration 1000 / 1500: loss 16.655648\n",
            "iteration 1100 / 1500: loss 14.310518\n",
            "iteration 1200 / 1500: loss 11.781303\n",
            "iteration 1300 / 1500: loss 22.109339\n",
            "iteration 1400 / 1500: loss 15.882025\n",
            "iteration 0 / 1500: loss 531.685537\n",
            "iteration 100 / 1500: loss 20.402381\n",
            "iteration 200 / 1500: loss 20.655794\n",
            "iteration 300 / 1500: loss 16.566120\n",
            "iteration 400 / 1500: loss 18.562067\n",
            "iteration 500 / 1500: loss 25.219717\n",
            "iteration 600 / 1500: loss 24.142863\n",
            "iteration 700 / 1500: loss 22.628832\n",
            "iteration 800 / 1500: loss 21.612468\n",
            "iteration 900 / 1500: loss 30.098054\n",
            "iteration 1000 / 1500: loss 22.200605\n",
            "iteration 1100 / 1500: loss 24.658288\n",
            "iteration 1200 / 1500: loss 22.247804\n",
            "iteration 1300 / 1500: loss 26.358004\n",
            "iteration 1400 / 1500: loss 22.513575\n",
            "iteration 0 / 1500: loss 672.747869\n",
            "iteration 100 / 1500: loss 23.062147\n",
            "iteration 200 / 1500: loss 22.298545\n",
            "iteration 300 / 1500: loss 34.687272\n",
            "iteration 400 / 1500: loss 22.949815\n",
            "iteration 500 / 1500: loss 26.744663\n",
            "iteration 600 / 1500: loss 27.291554\n",
            "iteration 700 / 1500: loss 24.708585\n",
            "iteration 800 / 1500: loss 24.714663\n",
            "iteration 900 / 1500: loss 31.889919\n",
            "iteration 1000 / 1500: loss 25.140781\n",
            "iteration 1100 / 1500: loss 24.936187\n",
            "iteration 1200 / 1500: loss 23.319689\n",
            "iteration 1300 / 1500: loss 26.564332\n",
            "iteration 1400 / 1500: loss 26.350021\n",
            "iteration 0 / 1500: loss 811.530755\n",
            "iteration 100 / 1500: loss 35.521047\n",
            "iteration 200 / 1500: loss 34.410563\n",
            "iteration 300 / 1500: loss 29.948285\n",
            "iteration 400 / 1500: loss 31.467384\n",
            "iteration 500 / 1500: loss 31.018193\n",
            "iteration 600 / 1500: loss 35.752473\n",
            "iteration 700 / 1500: loss 32.172885\n",
            "iteration 800 / 1500: loss 30.053296\n",
            "iteration 900 / 1500: loss 40.614011\n",
            "iteration 1000 / 1500: loss 35.171271\n",
            "iteration 1100 / 1500: loss 35.028353\n",
            "iteration 1200 / 1500: loss 31.154486\n",
            "iteration 1300 / 1500: loss 33.710359\n",
            "iteration 1400 / 1500: loss 34.947418\n",
            "iteration 0 / 1500: loss 958.289362\n",
            "iteration 100 / 1500: loss 55.605150\n",
            "iteration 200 / 1500: loss 56.652532\n",
            "iteration 300 / 1500: loss 58.885821\n",
            "iteration 400 / 1500: loss 59.081156\n",
            "iteration 500 / 1500: loss 51.024487\n",
            "iteration 600 / 1500: loss 50.818374\n",
            "iteration 700 / 1500: loss 50.522975\n",
            "iteration 800 / 1500: loss 49.653783\n",
            "iteration 900 / 1500: loss 54.083339\n",
            "iteration 1000 / 1500: loss 52.125859\n",
            "iteration 1100 / 1500: loss 54.721832\n",
            "iteration 1200 / 1500: loss 57.707806\n",
            "iteration 1300 / 1500: loss 54.907846\n",
            "iteration 1400 / 1500: loss 51.023957\n",
            "iteration 0 / 1500: loss 1096.948199\n",
            "iteration 100 / 1500: loss 83.348201\n",
            "iteration 200 / 1500: loss 107.479200\n",
            "iteration 300 / 1500: loss 105.672018\n",
            "iteration 400 / 1500: loss 99.867728\n",
            "iteration 500 / 1500: loss 97.869933\n",
            "iteration 600 / 1500: loss 85.418220\n",
            "iteration 700 / 1500: loss 88.825728\n",
            "iteration 800 / 1500: loss 94.970958\n",
            "iteration 900 / 1500: loss 101.978800\n",
            "iteration 1000 / 1500: loss 87.798735\n",
            "iteration 1100 / 1500: loss 96.962964\n",
            "iteration 1200 / 1500: loss 95.846493\n",
            "iteration 1300 / 1500: loss 98.655773\n",
            "iteration 1400 / 1500: loss 85.974811\n",
            "iteration 0 / 1500: loss 1236.257610\n",
            "iteration 100 / 1500: loss 214.594738\n",
            "iteration 200 / 1500: loss 203.226621\n",
            "iteration 300 / 1500: loss 187.350935\n",
            "iteration 400 / 1500: loss 189.050696\n",
            "iteration 500 / 1500: loss 204.316911\n",
            "iteration 600 / 1500: loss 219.496291\n",
            "iteration 700 / 1500: loss 193.591349\n",
            "iteration 800 / 1500: loss 193.327947\n",
            "iteration 900 / 1500: loss 213.685283\n",
            "iteration 1000 / 1500: loss 213.623818\n",
            "iteration 1100 / 1500: loss 225.922375\n",
            "iteration 1200 / 1500: loss 191.755666\n",
            "iteration 1300 / 1500: loss 194.945636\n",
            "iteration 1400 / 1500: loss 197.950262\n",
            "iteration 0 / 1500: loss 1410.070521\n",
            "iteration 100 / 1500: loss 833.281279\n",
            "iteration 200 / 1500: loss 750.656213\n",
            "iteration 300 / 1500: loss 814.892198\n",
            "iteration 400 / 1500: loss 741.113922\n",
            "iteration 500 / 1500: loss 766.680079\n",
            "iteration 600 / 1500: loss 808.799081\n",
            "iteration 700 / 1500: loss 805.166254\n",
            "iteration 800 / 1500: loss 734.721536\n",
            "iteration 900 / 1500: loss 692.294076\n",
            "iteration 1000 / 1500: loss 764.880032\n",
            "iteration 1100 / 1500: loss 775.819097\n",
            "iteration 1200 / 1500: loss 771.784334\n",
            "iteration 1300 / 1500: loss 837.432601\n",
            "iteration 1400 / 1500: loss 796.155521\n",
            "iteration 0 / 1500: loss 82.322069\n",
            "iteration 100 / 1500: loss 9.516678\n",
            "iteration 200 / 1500: loss 9.099056\n",
            "iteration 300 / 1500: loss 10.613213\n",
            "iteration 400 / 1500: loss 11.113855\n",
            "iteration 500 / 1500: loss 12.924893\n",
            "iteration 600 / 1500: loss 10.102326\n",
            "iteration 700 / 1500: loss 7.262859\n",
            "iteration 800 / 1500: loss 10.266613\n",
            "iteration 900 / 1500: loss 16.412439\n",
            "iteration 1000 / 1500: loss 12.614234\n",
            "iteration 1100 / 1500: loss 10.336860\n",
            "iteration 1200 / 1500: loss 10.296960\n",
            "iteration 1300 / 1500: loss 10.707834\n",
            "iteration 1400 / 1500: loss 6.871410\n",
            "iteration 0 / 1500: loss 229.681481\n",
            "iteration 100 / 1500: loss 14.946224\n",
            "iteration 200 / 1500: loss 18.954894\n",
            "iteration 300 / 1500: loss 17.950746\n",
            "iteration 400 / 1500: loss 21.572948\n",
            "iteration 500 / 1500: loss 15.953716\n",
            "iteration 600 / 1500: loss 17.143880\n",
            "iteration 700 / 1500: loss 14.967915\n",
            "iteration 800 / 1500: loss 18.974252\n",
            "iteration 900 / 1500: loss 17.964734\n",
            "iteration 1000 / 1500: loss 27.856464\n",
            "iteration 1100 / 1500: loss 18.296648\n",
            "iteration 1200 / 1500: loss 12.844412\n",
            "iteration 1300 / 1500: loss 15.073192\n",
            "iteration 1400 / 1500: loss 23.090282\n",
            "iteration 0 / 1500: loss 371.303253\n",
            "iteration 100 / 1500: loss 17.526925\n",
            "iteration 200 / 1500: loss 19.523895\n",
            "iteration 300 / 1500: loss 27.649013\n",
            "iteration 400 / 1500: loss 25.442364\n",
            "iteration 500 / 1500: loss 22.794136\n",
            "iteration 600 / 1500: loss 25.025150\n",
            "iteration 700 / 1500: loss 29.063034\n",
            "iteration 800 / 1500: loss 26.382734\n",
            "iteration 900 / 1500: loss 24.567780\n",
            "iteration 1000 / 1500: loss 26.234651\n",
            "iteration 1100 / 1500: loss 23.618807\n",
            "iteration 1200 / 1500: loss 24.785470\n",
            "iteration 1300 / 1500: loss 24.348965\n",
            "iteration 1400 / 1500: loss 24.515380\n",
            "iteration 0 / 1500: loss 520.890463\n",
            "iteration 100 / 1500: loss 25.044954\n",
            "iteration 200 / 1500: loss 31.373299\n",
            "iteration 300 / 1500: loss 37.471689\n",
            "iteration 400 / 1500: loss 33.906822\n",
            "iteration 500 / 1500: loss 30.856233\n",
            "iteration 600 / 1500: loss 31.664770\n",
            "iteration 700 / 1500: loss 32.707203\n",
            "iteration 800 / 1500: loss 27.275630\n",
            "iteration 900 / 1500: loss 26.390043\n",
            "iteration 1000 / 1500: loss 26.036326\n",
            "iteration 1100 / 1500: loss 33.762789\n",
            "iteration 1200 / 1500: loss 31.775090\n",
            "iteration 1300 / 1500: loss 29.645041\n",
            "iteration 1400 / 1500: loss 36.352992\n",
            "iteration 0 / 1500: loss 671.802394\n",
            "iteration 100 / 1500: loss 47.543212\n",
            "iteration 200 / 1500: loss 45.331501\n",
            "iteration 300 / 1500: loss 45.933670\n",
            "iteration 400 / 1500: loss 60.234580\n",
            "iteration 500 / 1500: loss 56.078818\n",
            "iteration 600 / 1500: loss 50.614658\n",
            "iteration 700 / 1500: loss 47.618412\n",
            "iteration 800 / 1500: loss 55.031959\n",
            "iteration 900 / 1500: loss 51.445320\n",
            "iteration 1000 / 1500: loss 55.001825\n",
            "iteration 1100 / 1500: loss 47.707224\n",
            "iteration 1200 / 1500: loss 46.513590\n",
            "iteration 1300 / 1500: loss 52.249250\n",
            "iteration 1400 / 1500: loss 56.410656\n",
            "iteration 0 / 1500: loss 819.188061\n",
            "iteration 100 / 1500: loss 73.015546\n",
            "iteration 200 / 1500: loss 99.501970\n",
            "iteration 300 / 1500: loss 82.093959\n",
            "iteration 400 / 1500: loss 87.572566\n",
            "iteration 500 / 1500: loss 86.146252\n",
            "iteration 600 / 1500: loss 77.415993\n",
            "iteration 700 / 1500: loss 89.019409\n",
            "iteration 800 / 1500: loss 83.848955\n",
            "iteration 900 / 1500: loss 104.208272\n",
            "iteration 1000 / 1500: loss 81.215596\n",
            "iteration 1100 / 1500: loss 83.850560\n",
            "iteration 1200 / 1500: loss 83.879882\n",
            "iteration 1300 / 1500: loss 87.919433\n",
            "iteration 1400 / 1500: loss 72.653066\n",
            "iteration 0 / 1500: loss 975.981619\n",
            "iteration 100 / 1500: loss 185.547135\n",
            "iteration 200 / 1500: loss 207.630959\n",
            "iteration 300 / 1500: loss 169.301939\n",
            "iteration 400 / 1500: loss 193.777294\n",
            "iteration 500 / 1500: loss 183.361003\n",
            "iteration 600 / 1500: loss 185.425795\n",
            "iteration 700 / 1500: loss 172.698084\n",
            "iteration 800 / 1500: loss 188.942036\n",
            "iteration 900 / 1500: loss 165.896837\n",
            "iteration 1000 / 1500: loss 178.715913\n",
            "iteration 1100 / 1500: loss 158.399522\n",
            "iteration 1200 / 1500: loss 203.286117\n",
            "iteration 1300 / 1500: loss 186.286463\n",
            "iteration 1400 / 1500: loss 191.516351\n",
            "iteration 0 / 1500: loss 1112.352818\n",
            "iteration 100 / 1500: loss 765.528692\n",
            "iteration 200 / 1500: loss 755.295558\n",
            "iteration 300 / 1500: loss 730.565125\n",
            "iteration 400 / 1500: loss 759.272210\n",
            "iteration 500 / 1500: loss 754.528515\n",
            "iteration 600 / 1500: loss 757.334977\n",
            "iteration 700 / 1500: loss 759.975567\n",
            "iteration 800 / 1500: loss 751.529796\n",
            "iteration 900 / 1500: loss 734.856609\n",
            "iteration 1000 / 1500: loss 774.800316\n",
            "iteration 1100 / 1500: loss 786.484357\n",
            "iteration 1200 / 1500: loss 780.345020\n",
            "iteration 1300 / 1500: loss 768.825626\n",
            "iteration 1400 / 1500: loss 781.070846\n",
            "iteration 0 / 1500: loss 1233.121767\n",
            "iteration 100 / 1500: loss 17264252.866771\n",
            "iteration 200 / 1500: loss 12500954101.115456\n",
            "iteration 300 / 1500: loss 8445015431838.742188\n",
            "iteration 400 / 1500: loss 5690077827228585.000000\n",
            "iteration 500 / 1500: loss 3833471616361616896.000000\n",
            "iteration 600 / 1500: loss 2582644491986947014656.000000\n",
            "iteration 700 / 1500: loss 1739950687777584121380864.000000\n",
            "iteration 800 / 1500: loss 1172220330000894838511239168.000000\n",
            "iteration 900 / 1500: loss 789735313429448042239550816256.000000\n",
            "iteration 1000 / 1500: loss 532051739174098965012586396909568.000000\n",
            "iteration 1100 / 1500: loss 358448012067245093823880993966129152.000000\n",
            "iteration 1200 / 1500: loss 241489629475519687598281552876378521600.000000\n",
            "iteration 1300 / 1500: loss 162693721769848828774827009117054522359808.000000\n",
            "iteration 1400 / 1500: loss 109608214484457687764149028386982032840851456.000000\n",
            "iteration 0 / 1500: loss 1402.927117\n",
            "iteration 100 / 1500: loss 1284456833908351248105472.000000\n",
            "iteration 200 / 1500: loss 944948100459388102516104763232692625217683456.000000\n",
            "iteration 300 / 1500: loss 695178607000799392904466787870595647699783632614673416083921174528.000000\n",
            "iteration 400 / 1500: loss 511428400561499316877669138844612116102262212530176248936110736934873447437372250652672.000000\n",
            "iteration 500 / 1500: loss 376247206497527774427407020691868895262365575619254242468099049017947299867451358446826179677007834924449792.000000\n",
            "iteration 600 / 1500: loss 276797221745549891558787222623723892610359250278663222068747423538608206260362489824624279790493030526849729277839716098884239360.000000\n",
            "iteration 700 / 1500: loss 203633942373359686861218809217988940554993955403565728471907094199755433561451389900601439151843775771008271170653496358270117593191924256074176135168.000000\n",
            "iteration 800 / 1500: loss 149809243839288813952693344338070046561350401935473144890524309330940158052335631310848338499046621549426794252877921003186871090159513971816399663020344616050447083372544.000000\n",
            "iteration 900 / 1500: loss 110211535847746570872459403794769706683074904351833627666548227271920328259957166996163264824444542211509739243627173218361859911762370735826907676761096366215318041077320784332696812529909760.000000\n",
            "iteration 1000 / 1500: loss 81080328040034972029092384589605270759097425442290342386398839209354282085617073693181124874969881538662403610470248687484080608762709139591678185548708391928969568066261673036046172114720830407330728106702929920.000000\n",
            "iteration 1100 / 1500: loss 59649106098670684207140673691240726293220053013183912367160416566779809444860995758912263884671509621564885333731798177328658840824569161404962187970918000027774411137265440296976217655517165444988154979813766449931915006496426950656.000000\n",
            "iteration 1200 / 1500: loss 43882603146519503670588895741363727774772012195074318670602003688920406434815399553651772721404443683781966850275162970374920073914619834036107242913553924976324363351542640984121206655649648837175552544190664819096877048612403816817289957993147457863680.000000\n",
            "iteration 1300 / 1500: loss 32283515795349678679676338554327785642674648858417300310683347071031426230475307118868135411197591123353243243205829876431210513401047539812752119448943263904513555603343015959214183402280073972965848189238807622978141544474330414722873817971499977698519400456727994705117184.000000\n",
            "iteration 1400 / 1500: loss 23750309174428607539376473770316168599194790848117736138759190202677364937437884015341286744331699439033923128657273197121268305933292384921439903732656550450365863700935849349828245559421001307708626242242876236646877513931368752493453822396211100347564738072211620727377150910132643401694183424.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/CS231n/assignment1/cs231n/classifiers/softmax.py:144: RuntimeWarning: overflow encountered in double_scalars\n",
            "  loss += reg * np.sum(W * W)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:87: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 0 / 1500: loss 82.516719\n",
            "iteration 100 / 1500: loss 15.586025\n",
            "iteration 200 / 1500: loss 14.824774\n",
            "iteration 300 / 1500: loss 17.023499\n",
            "iteration 400 / 1500: loss 16.715899\n",
            "iteration 500 / 1500: loss 13.274775\n",
            "iteration 600 / 1500: loss 14.365559\n",
            "iteration 700 / 1500: loss 15.360068\n",
            "iteration 800 / 1500: loss 19.570159\n",
            "iteration 900 / 1500: loss 18.684636\n",
            "iteration 1000 / 1500: loss 15.774418\n",
            "iteration 1100 / 1500: loss 18.941955\n",
            "iteration 1200 / 1500: loss 17.360462\n",
            "iteration 1300 / 1500: loss 18.068443\n",
            "iteration 1400 / 1500: loss 18.886042\n",
            "iteration 0 / 1500: loss 232.229982\n",
            "iteration 100 / 1500: loss 18.975661\n",
            "iteration 200 / 1500: loss 19.432746\n",
            "iteration 300 / 1500: loss 25.595246\n",
            "iteration 400 / 1500: loss 24.545157\n",
            "iteration 500 / 1500: loss 24.053125\n",
            "iteration 600 / 1500: loss 30.818643\n",
            "iteration 700 / 1500: loss 22.612371\n",
            "iteration 800 / 1500: loss 28.076314\n",
            "iteration 900 / 1500: loss 21.555255\n",
            "iteration 1000 / 1500: loss 22.461576\n",
            "iteration 1100 / 1500: loss 22.794243\n",
            "iteration 1200 / 1500: loss 27.946281\n",
            "iteration 1300 / 1500: loss 16.953658\n",
            "iteration 1400 / 1500: loss 18.927306\n",
            "iteration 0 / 1500: loss 376.637178\n",
            "iteration 100 / 1500: loss 25.565342\n",
            "iteration 200 / 1500: loss 29.359595\n",
            "iteration 300 / 1500: loss 26.995927\n",
            "iteration 400 / 1500: loss 25.260658\n",
            "iteration 500 / 1500: loss 31.368566\n",
            "iteration 600 / 1500: loss 29.876004\n",
            "iteration 700 / 1500: loss 34.056514\n",
            "iteration 800 / 1500: loss 27.007169\n",
            "iteration 900 / 1500: loss 40.539090\n",
            "iteration 1000 / 1500: loss 33.750996\n",
            "iteration 1100 / 1500: loss 25.411185\n",
            "iteration 1200 / 1500: loss 26.126520\n",
            "iteration 1300 / 1500: loss 25.230861\n",
            "iteration 1400 / 1500: loss 23.332149\n",
            "iteration 0 / 1500: loss 515.425033\n",
            "iteration 100 / 1500: loss 45.949057\n",
            "iteration 200 / 1500: loss 51.327624\n",
            "iteration 300 / 1500: loss 48.723363\n",
            "iteration 400 / 1500: loss 43.676100\n",
            "iteration 500 / 1500: loss 49.235061\n",
            "iteration 600 / 1500: loss 46.365760\n",
            "iteration 700 / 1500: loss 48.296711\n",
            "iteration 800 / 1500: loss 56.673330\n",
            "iteration 900 / 1500: loss 43.653564\n",
            "iteration 1000 / 1500: loss 49.931366\n",
            "iteration 1100 / 1500: loss 50.623991\n",
            "iteration 1200 / 1500: loss 44.282015\n",
            "iteration 1300 / 1500: loss 47.663617\n",
            "iteration 1400 / 1500: loss 48.106477\n",
            "iteration 0 / 1500: loss 663.954525\n",
            "iteration 100 / 1500: loss 91.163282\n",
            "iteration 200 / 1500: loss 97.940531\n",
            "iteration 300 / 1500: loss 83.694805\n",
            "iteration 400 / 1500: loss 94.995937\n",
            "iteration 500 / 1500: loss 92.739206\n",
            "iteration 600 / 1500: loss 107.004686\n",
            "iteration 700 / 1500: loss 112.873413\n",
            "iteration 800 / 1500: loss 94.829071\n",
            "iteration 900 / 1500: loss 95.217740\n",
            "iteration 1000 / 1500: loss 94.616332\n",
            "iteration 1100 / 1500: loss 89.588723\n",
            "iteration 1200 / 1500: loss 90.157966\n",
            "iteration 1300 / 1500: loss 92.260667\n",
            "iteration 1400 / 1500: loss 80.870966\n",
            "iteration 0 / 1500: loss 813.979371\n",
            "iteration 100 / 1500: loss 230.741280\n",
            "iteration 200 / 1500: loss 242.931306\n",
            "iteration 300 / 1500: loss 212.467269\n",
            "iteration 400 / 1500: loss 246.816679\n",
            "iteration 500 / 1500: loss 225.879265\n",
            "iteration 600 / 1500: loss 240.478954\n",
            "iteration 700 / 1500: loss 223.833313\n",
            "iteration 800 / 1500: loss 223.410528\n",
            "iteration 900 / 1500: loss 241.168666\n",
            "iteration 1000 / 1500: loss 245.808543\n",
            "iteration 1100 / 1500: loss 238.765547\n",
            "iteration 1200 / 1500: loss 261.096531\n",
            "iteration 1300 / 1500: loss 240.924591\n",
            "iteration 1400 / 1500: loss 202.245889\n",
            "iteration 0 / 1500: loss 965.281094\n",
            "iteration 100 / 1500: loss 2144.328626\n",
            "iteration 200 / 1500: loss 2127.749156\n",
            "iteration 300 / 1500: loss 2049.963323\n",
            "iteration 400 / 1500: loss 2173.098736\n",
            "iteration 500 / 1500: loss 2061.608303\n",
            "iteration 600 / 1500: loss 2199.827029\n",
            "iteration 700 / 1500: loss 2084.845416\n",
            "iteration 800 / 1500: loss 2075.564104\n",
            "iteration 900 / 1500: loss 2010.624366\n",
            "iteration 1000 / 1500: loss 2209.037743\n",
            "iteration 1100 / 1500: loss 2060.565596\n",
            "iteration 1200 / 1500: loss 2192.118096\n",
            "iteration 1300 / 1500: loss 2092.759185\n",
            "iteration 1400 / 1500: loss 2106.568558\n",
            "iteration 0 / 1500: loss 1111.785214\n",
            "iteration 100 / 1500: loss 5061726406563595.000000\n",
            "iteration 200 / 1500: loss 10141538561242656948818542592.000000\n",
            "iteration 300 / 1500: loss 20319297062594245608904648587225904185344.000000\n",
            "iteration 400 / 1500: loss 40711163362882810905749539046878916007304351549751296.000000\n",
            "iteration 500 / 1500: loss 81567724378144662216354901201842459651509526474785734635003314176.000000\n",
            "iteration 600 / 1500: loss 163426763340664342390267181029289276967619371713158871058211009938384157147136.000000\n",
            "iteration 700 / 1500: loss 327437196263890920126766730328253565666859362661761186457649599743511147364528615296335872.000000\n",
            "iteration 800 / 1500: loss 656043816236310645855538767814969331352425875483660394497203398508406636127343914958636608513455947776.000000\n",
            "iteration 900 / 1500: loss 1314430656421317794165230493073584049114507483989424201282262804722536597907460024538104927641189700120867606888448.000000\n",
            "iteration 1000 / 1500: loss 2633555728719558027791941192861426884134236853878254236086153915834485700274897523092921872064342324379040502358169720201412608.000000\n",
            "iteration 1100 / 1500: loss 5276517055037789692683953420926203558079705075636789457165391466786598464428840230668074717398120782485708382041379392373077625719350099968.000000\n",
            "iteration 1200 / 1500: loss 10571878896840868289185801404387212853902425836624802645259571128050859416666283794695452794136735178635669819478754661423379941313779254094656501186560.000000\n",
            "iteration 1300 / 1500: loss 21181514670318615663175479018320386693486506898792559334099005210989121579580073256846159513665413584363426806852481871198369862641202176621868986179626763210981376.000000\n",
            "iteration 1400 / 1500: loss 42438677940492869882744941515768285725767692506180289211730439504479145916429246242482868188136177325735259019563613973968755828137076222705613790881097580072384885851879899136.000000\n",
            "iteration 0 / 1500: loss 1248.742271\n",
            "iteration 100 / 1500: loss 49124807908329543487339207887683584.000000\n",
            "iteration 200 / 1500: loss 1774672099444162430259026824925847348125598170240667429967243509760.000000\n",
            "iteration 300 / 1500: loss 64111417319385222755912571683028955217225292776870030057749329603651560595563729116890628534304768.000000\n",
            "iteration 400 / 1500: loss 2316075083384547601685383262519130582359546038650331343473188508579127630232112501847182400146286989241170413615345581762264694784.000000\n",
            "iteration 500 / 1500: loss 83670023471042115439972333609232504980054288804641902578391298793916275772809708497418665470842762959396024208710457038835072754837093117635325841164926107779072.000000\n",
            "iteration 600 / 1500: loss 3022645024708980479626331456629840542844319578048395786721817477267076193661101933983048982028865573381483208849126430533120970287375310088795392434055443739860479387605114531263011980465668096.000000\n",
            "iteration 700 / 1500: loss 109195415112558448327113444332868232454674855189212079787527467660554517061839850735421877984294494962442738306002754112152662627795702850853998427219040332388980738587641477137470021068216519518549802622173723586409904734208.000000\n",
            "iteration 800 / 1500: loss 3944769757656859537124334102355521684632651948318732839243078472701089828916741710432802363385136823613727033789888202370400684366854042628380786050128180629619338149743969508453536243513954764577046148515098675878090946192745371994348324557971785307389952.000000\n",
            "iteration 900 / 1500: loss 142507892157227413697659440292563060521318247825342426276307108573759254923744451955532235248293940957895470397246099344781077735626773623410385689396316701567072713177135625236964090161578536808558386141721299396602653571623185913545175972934705641522981853002833637781977345867630247936.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/CS231n/assignment1/cs231n/classifiers/softmax.py:144: RuntimeWarning: overflow encountered in multiply\n",
            "  loss += reg * np.sum(W * W)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 1000 / 1500: loss inf\n",
            "iteration 1100 / 1500: loss inf\n",
            "iteration 1200 / 1500: loss inf\n",
            "iteration 1300 / 1500: loss inf\n",
            "iteration 1400 / 1500: loss inf\n",
            "iteration 0 / 1500: loss 1390.548734\n",
            "iteration 100 / 1500: loss 298850610225242382447286307731146501135353119768576.000000\n",
            "iteration 200 / 1500: loss 61013440849154963062606554396821897074899749956595360107360856640697344274470806947140902393479168.000000\n",
            "iteration 300 / 1500: loss 12456524554015784065148736182480759218814092325088889277868611054600228112155125224103064663367120991625376056987708814779795946072065271201267712.000000\n",
            "iteration 400 / 1500: loss 2543128232161441152863282779441402758069874378873993221839645989673600809317040839065606078523899371026860119989892239434425999042851426125911608018164262051187326295000372174776457939744980992.000000\n",
            "iteration 500 / 1500: loss 519205913107726242422781701050284186994849502260997236913888068929844597242637582760537411844115014965567150239473739746665005780610726356361906142146961897486170648230152144356177844884092534682756188008375271773540091283550424076130451456.000000\n",
            "iteration 600 / 1500: loss 106001253415724225371443753263682248898482961726039534360575708059330075613281548130883045793086697057915707566077200826622854835882077401413584825482019783330553031545666612216381796161627587210954752289875894539774659153993672947737405075620286980703621388828920209516532285827746627584.000000\n",
            "iteration 700 / 1500: loss inf\n",
            "iteration 800 / 1500: loss inf\n",
            "iteration 900 / 1500: loss inf\n",
            "iteration 1000 / 1500: loss inf\n",
            "iteration 1100 / 1500: loss inf\n",
            "iteration 1200 / 1500: loss inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/CS231n/assignment1/cs231n/classifiers/softmax.py:132: RuntimeWarning: overflow encountered in subtract\n",
            "  L -= np.max(L, axis=1).reshape(-1, 1) # numeric instability...\n",
            "/content/drive/My Drive/CS231n/assignment1/cs231n/classifiers/softmax.py:147: RuntimeWarning: overflow encountered in multiply\n",
            "  dW += 2 * reg * W\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 1300 / 1500: loss nan\n",
            "iteration 1400 / 1500: loss nan\n",
            "iteration 0 / 1500: loss 81.897384\n",
            "iteration 100 / 1500: loss 21.873933\n",
            "iteration 200 / 1500: loss 14.158145\n",
            "iteration 300 / 1500: loss 17.609833\n",
            "iteration 400 / 1500: loss 22.501826\n",
            "iteration 500 / 1500: loss 27.225090\n",
            "iteration 600 / 1500: loss 18.642125\n",
            "iteration 700 / 1500: loss 17.174971\n",
            "iteration 800 / 1500: loss 22.030013\n",
            "iteration 900 / 1500: loss 29.569648\n",
            "iteration 1000 / 1500: loss 11.191314\n",
            "iteration 1100 / 1500: loss 22.045953\n",
            "iteration 1200 / 1500: loss 12.365931\n",
            "iteration 1300 / 1500: loss 10.988225\n",
            "iteration 1400 / 1500: loss 21.849853\n",
            "iteration 0 / 1500: loss 230.533099\n",
            "iteration 100 / 1500: loss 25.062689\n",
            "iteration 200 / 1500: loss 25.288238\n",
            "iteration 300 / 1500: loss 31.202721\n",
            "iteration 400 / 1500: loss 33.707202\n",
            "iteration 500 / 1500: loss 23.682917\n",
            "iteration 600 / 1500: loss 28.648055\n",
            "iteration 700 / 1500: loss 30.478021\n",
            "iteration 800 / 1500: loss 23.425595\n",
            "iteration 900 / 1500: loss 20.385792\n",
            "iteration 1000 / 1500: loss 25.101297\n",
            "iteration 1100 / 1500: loss 29.913051\n",
            "iteration 1200 / 1500: loss 16.433776\n",
            "iteration 1300 / 1500: loss 31.012892\n",
            "iteration 1400 / 1500: loss 34.530285\n",
            "iteration 0 / 1500: loss 376.231326\n",
            "iteration 100 / 1500: loss 37.295444\n",
            "iteration 200 / 1500: loss 45.979212\n",
            "iteration 300 / 1500: loss 47.389916\n",
            "iteration 400 / 1500: loss 50.281110\n",
            "iteration 500 / 1500: loss 52.010978\n",
            "iteration 600 / 1500: loss 46.090806\n",
            "iteration 700 / 1500: loss 44.608759\n",
            "iteration 800 / 1500: loss 38.305978\n",
            "iteration 900 / 1500: loss 42.851935\n",
            "iteration 1000 / 1500: loss 49.914411\n",
            "iteration 1100 / 1500: loss 46.743306\n",
            "iteration 1200 / 1500: loss 40.337430\n",
            "iteration 1300 / 1500: loss 50.727601\n",
            "iteration 1400 / 1500: loss 38.482768\n",
            "iteration 0 / 1500: loss 521.309679\n",
            "iteration 100 / 1500: loss 74.237429\n",
            "iteration 200 / 1500: loss 92.713888\n",
            "iteration 300 / 1500: loss 94.546790\n",
            "iteration 400 / 1500: loss 73.240629\n",
            "iteration 500 / 1500: loss 88.225180\n",
            "iteration 600 / 1500: loss 72.932921\n",
            "iteration 700 / 1500: loss 89.991711\n",
            "iteration 800 / 1500: loss 77.128882\n",
            "iteration 900 / 1500: loss 78.206825\n",
            "iteration 1000 / 1500: loss 79.018739\n",
            "iteration 1100 / 1500: loss 90.737366\n",
            "iteration 1200 / 1500: loss 83.720389\n",
            "iteration 1300 / 1500: loss 64.270159\n",
            "iteration 1400 / 1500: loss 86.493655\n",
            "iteration 0 / 1500: loss 655.228135\n",
            "iteration 100 / 1500: loss 206.536212\n",
            "iteration 200 / 1500: loss 214.935219\n",
            "iteration 300 / 1500: loss 215.929692\n",
            "iteration 400 / 1500: loss 196.305464\n",
            "iteration 500 / 1500: loss 201.959343\n",
            "iteration 600 / 1500: loss 241.754302\n",
            "iteration 700 / 1500: loss 206.648996\n",
            "iteration 800 / 1500: loss 219.570456\n",
            "iteration 900 / 1500: loss 219.055939\n",
            "iteration 1000 / 1500: loss 188.427189\n",
            "iteration 1100 / 1500: loss 214.198610\n",
            "iteration 1200 / 1500: loss 218.448939\n",
            "iteration 1300 / 1500: loss 235.015578\n",
            "iteration 1400 / 1500: loss 209.480752\n",
            "iteration 0 / 1500: loss 807.182398\n",
            "iteration 100 / 1500: loss 1936.356000\n",
            "iteration 200 / 1500: loss 1833.264469\n",
            "iteration 300 / 1500: loss 1825.619534\n",
            "iteration 400 / 1500: loss 1728.157390\n",
            "iteration 500 / 1500: loss 1780.383520\n",
            "iteration 600 / 1500: loss 1806.265570\n",
            "iteration 700 / 1500: loss 1785.481140\n",
            "iteration 800 / 1500: loss 1735.953548\n",
            "iteration 900 / 1500: loss 1795.065063\n",
            "iteration 1000 / 1500: loss 1671.021731\n",
            "iteration 1100 / 1500: loss 1747.491381\n",
            "iteration 1200 / 1500: loss 1797.907386\n",
            "iteration 1300 / 1500: loss 1869.474269\n",
            "iteration 1400 / 1500: loss 1758.872321\n",
            "iteration 0 / 1500: loss 951.989305\n",
            "iteration 100 / 1500: loss 252117100288909280.000000\n",
            "iteration 200 / 1500: loss 31465697197352271446034874368000.000000\n",
            "iteration 300 / 1500: loss 3927103724591610628478140217761294580208107520.000000\n",
            "iteration 400 / 1500: loss 490125598265748697621193504738350352577571960845343599362048.000000\n",
            "iteration 500 / 1500: loss 61170551867799095941801424974001049679472149391635433935202154000842489856.000000\n",
            "iteration 600 / 1500: loss 7634443965079856902920729083939946210144883700643354345251595131629526029057630071160832.000000\n",
            "iteration 700 / 1500: loss 952823423628878679041724343619939623882403423233218236350333320907171171020805871382112865829076860928.000000\n",
            "iteration 800 / 1500: loss 118917956666980568551775687410777313862921978617016495132933554637680408187378587779702724041553619675520582134267904.000000\n",
            "iteration 900 / 1500: loss 14841659080956769120394197076751072303126261560064834011019155935344084306353428356711020311256739604506538180182954008111057207296.000000\n",
            "iteration 1000 / 1500: loss 1852326178898340433966923549636473442832521193283784561249027535356359093395351893282705560162584317439072969885488000221280505354054148619763712.000000\n",
            "iteration 1100 / 1500: loss 231181180912217779290898884686366417338592071058416302728889100780303140497501300531771697098673697535742073746764352235878012944044585424954181969898960322560.000000\n",
            "iteration 1200 / 1500: loss 28852768490133578423351087007775253899239384082385966843899014261411263626860704231856888157871066156579297646653766271310932370883312000063885934590376736373056302496088064.000000\n",
            "iteration 1300 / 1500: loss 3600994883149024477745378274041453866330753450944693682477125088407206536896373985755625939328023870321245493984987921027911895810363885301277474997532481770137540637932673341613947224064.000000\n",
            "iteration 1400 / 1500: loss 449425300483718790105645737921529539518151066167491386669398495643850506146475956803266523979521097685229580335995284200369423026156523120042269520152203820532322018298469222021364283502091790542962688.000000\n",
            "iteration 0 / 1500: loss 1101.604038\n",
            "iteration 100 / 1500: loss 715827028390377811320315980116719042560.000000\n",
            "iteration 200 / 1500: loss 426640776190235905177623059233378039080290552349883342174503475119805956096.000000\n",
            "iteration 300 / 1500: loss 254282591588509648979374868530176272117628569823205859898642356190383534378704874840340488200275925893004656640.000000\n",
            "iteration 400 / 1500: loss 151555219269846748206642794253159304811881422130631826308871941435683400927978561559641657007984708874303844752880598879382438263949877560479842304.000000\n",
            "iteration 500 / 1500: loss 90328576346668124249410037261822746607189894140884875580181528966536287109950624974700244875914598796149999980257088957836833703728444729796566990319767944118196779253741478356189184.000000\n",
            "iteration 600 / 1500: loss 53836824255376902811036433533612759181913129898350276345086109476473738152278332138181580248763972658979703411157556563306412104617140442180077422169403758477196474542047992621976876187310454575420779487651243199823872.000000\n",
            "iteration 700 / 1500: loss 32087338947762021153043892359599968427786989417630226171651495160389977486454806378840366559523789299709650874456853797742341811793439343118466906695123032371206926905408877555849257376080218133345384298142073209137373160686972144756806322056123830501376.000000\n",
            "iteration 800 / 1500: loss 19124406667537331413625483129691273616245325772384035577912764647342193397402271308311429675669771755449131750226184076087503088219517831915531676186467734231630245185971474092474556356209114800015203558560728651922911685629201954638552021531646759556723131437220744189179312335158830432256.000000\n",
            "iteration 900 / 1500: loss inf\n",
            "iteration 1000 / 1500: loss inf\n",
            "iteration 1100 / 1500: loss inf\n",
            "iteration 1200 / 1500: loss inf\n",
            "iteration 1300 / 1500: loss inf\n",
            "iteration 1400 / 1500: loss inf\n",
            "iteration 0 / 1500: loss 1257.516758\n",
            "iteration 100 / 1500: loss 167783027317004420980883478577256039653422148211072565248.000000\n",
            "iteration 200 / 1500: loss 21603685692314284719910669332513605976973997319736230361406211467421465372062478835328776799137307902106140672.000000\n",
            "iteration 300 / 1500: loss 2781683242670899444758896028493387426829620430881130869519765869283381761090386182205213650113378467472886502903502352163212273024314859638186684021331972086300672.000000\n",
            "iteration 400 / 1500: loss 358168590895065241750049078883122462577088580609312896058518019052278490423157560941155030468108353888580794450749194387826940463682061964763758800412914731084046527849113099623937536071738883223462831276128695484416.000000\n",
            "iteration 500 / 1500: loss 46117666287762126723863205196170174489888224881794059491836023047752112440964678498250061816684222141236664664555556743975199417637742336500643641234015722879056068138684852863261456702773614504220906329554751624220722770877602847401393946023277230742973201244936470528.000000\n",
            "iteration 600 / 1500: loss inf\n",
            "iteration 700 / 1500: loss inf\n",
            "iteration 800 / 1500: loss inf\n",
            "iteration 900 / 1500: loss inf\n",
            "iteration 1000 / 1500: loss inf\n",
            "iteration 1100 / 1500: loss inf\n",
            "iteration 1200 / 1500: loss nan\n",
            "iteration 1300 / 1500: loss nan\n",
            "iteration 1400 / 1500: loss nan\n",
            "iteration 0 / 1500: loss 1420.262375\n",
            "iteration 100 / 1500: loss 51577372644651924774476345736336395929142984461979642342808843460804608.000000\n",
            "iteration 200 / 1500: loss 1845817976353532680641481377233035359488116507714994798755461239236247990055193156369906470738986068760229061322206164076828242116429742080.000000\n",
            "iteration 300 / 1500: loss 66056951471783191975672554897345321469687075774200503153965178421777428258750020277644084940908682942915776804223807784300025055664369250782334374823905988298683735208665280798751945922935855129382023593984.000000\n",
            "iteration 400 / 1500: loss 2364003869095360310400282061309343296869317078534501389440444918471056037902755470986667558247455632310523605941713675851268432827152219928385858950877619364270605821411675278828791621212536245813378853258334908258535003759091224639198756229116185140203263571613249467056128.000000\n",
            "iteration 500 / 1500: loss inf\n",
            "iteration 600 / 1500: loss inf\n",
            "iteration 700 / 1500: loss inf\n",
            "iteration 800 / 1500: loss inf\n",
            "iteration 900 / 1500: loss inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/CS231n/assignment1/cs231n/classifiers/softmax.py:132: RuntimeWarning: invalid value encountered in subtract\n",
            "  L -= np.max(L, axis=1).reshape(-1, 1) # numeric instability...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 1000 / 1500: loss nan\n",
            "iteration 1100 / 1500: loss nan\n",
            "iteration 1200 / 1500: loss nan\n",
            "iteration 1300 / 1500: loss nan\n",
            "iteration 1400 / 1500: loss nan\n",
            "iteration 0 / 1500: loss 83.184695\n",
            "iteration 100 / 1500: loss 22.595893\n",
            "iteration 200 / 1500: loss 15.373243\n",
            "iteration 300 / 1500: loss 17.331851\n",
            "iteration 400 / 1500: loss 20.474211\n",
            "iteration 500 / 1500: loss 15.557711\n",
            "iteration 600 / 1500: loss 27.684752\n",
            "iteration 700 / 1500: loss 23.779163\n",
            "iteration 800 / 1500: loss 24.300351\n",
            "iteration 900 / 1500: loss 19.545508\n",
            "iteration 1000 / 1500: loss 17.157470\n",
            "iteration 1100 / 1500: loss 28.701954\n",
            "iteration 1200 / 1500: loss 15.186230\n",
            "iteration 1300 / 1500: loss 18.102048\n",
            "iteration 1400 / 1500: loss 11.431827\n",
            "iteration 0 / 1500: loss 228.498743\n",
            "iteration 100 / 1500: loss 32.306488\n",
            "iteration 200 / 1500: loss 38.670923\n",
            "iteration 300 / 1500: loss 59.223041\n",
            "iteration 400 / 1500: loss 43.925725\n",
            "iteration 500 / 1500: loss 26.639710\n",
            "iteration 600 / 1500: loss 36.082990\n",
            "iteration 700 / 1500: loss 42.343215\n",
            "iteration 800 / 1500: loss 42.040693\n",
            "iteration 900 / 1500: loss 34.549290\n",
            "iteration 1000 / 1500: loss 32.932954\n",
            "iteration 1100 / 1500: loss 42.590392\n",
            "iteration 1200 / 1500: loss 38.210042\n",
            "iteration 1300 / 1500: loss 41.999297\n",
            "iteration 1400 / 1500: loss 30.568045\n",
            "iteration 0 / 1500: loss 373.007235\n",
            "iteration 100 / 1500: loss 64.355874\n",
            "iteration 200 / 1500: loss 63.276389\n",
            "iteration 300 / 1500: loss 71.078368\n",
            "iteration 400 / 1500: loss 71.656786\n",
            "iteration 500 / 1500: loss 49.718242\n",
            "iteration 600 / 1500: loss 67.339987\n",
            "iteration 700 / 1500: loss 63.080524\n",
            "iteration 800 / 1500: loss 64.441248\n",
            "iteration 900 / 1500: loss 59.588447\n",
            "iteration 1000 / 1500: loss 58.986294\n",
            "iteration 1100 / 1500: loss 65.036220\n",
            "iteration 1200 / 1500: loss 60.840558\n",
            "iteration 1300 / 1500: loss 66.387076\n",
            "iteration 1400 / 1500: loss 51.920182\n",
            "iteration 0 / 1500: loss 516.463809\n",
            "iteration 100 / 1500: loss 156.473380\n",
            "iteration 200 / 1500: loss 146.017083\n",
            "iteration 300 / 1500: loss 151.172887\n",
            "iteration 400 / 1500: loss 139.568646\n",
            "iteration 500 / 1500: loss 158.233412\n",
            "iteration 600 / 1500: loss 133.933949\n",
            "iteration 700 / 1500: loss 144.086056\n",
            "iteration 800 / 1500: loss 134.896161\n",
            "iteration 900 / 1500: loss 136.597932\n",
            "iteration 1000 / 1500: loss 148.122117\n",
            "iteration 1100 / 1500: loss 135.061672\n",
            "iteration 1200 / 1500: loss 144.800703\n",
            "iteration 1300 / 1500: loss 147.522996\n",
            "iteration 1400 / 1500: loss 145.919982\n",
            "iteration 0 / 1500: loss 671.596820\n",
            "iteration 100 / 1500: loss 686.476216\n",
            "iteration 200 / 1500: loss 723.628164\n",
            "iteration 300 / 1500: loss 737.403989\n",
            "iteration 400 / 1500: loss 646.925060\n",
            "iteration 500 / 1500: loss 728.789538\n",
            "iteration 600 / 1500: loss 657.488084\n",
            "iteration 700 / 1500: loss 715.063878\n",
            "iteration 800 / 1500: loss 712.549270\n",
            "iteration 900 / 1500: loss 689.235450\n",
            "iteration 1000 / 1500: loss 667.060597\n",
            "iteration 1100 / 1500: loss 661.247130\n",
            "iteration 1200 / 1500: loss 658.520888\n",
            "iteration 1300 / 1500: loss 670.046364\n",
            "iteration 1400 / 1500: loss 652.530788\n",
            "iteration 0 / 1500: loss 813.835198\n",
            "iteration 100 / 1500: loss 2146144902650.838867\n",
            "iteration 200 / 1500: loss 1056324888865568849920.000000\n",
            "iteration 300 / 1500: loss 519878945935789374531740631040.000000\n",
            "iteration 400 / 1500: loss 255862680455824811741536539888781885440.000000\n",
            "iteration 500 / 1500: loss 125924913408814086865164666540716771360383172608.000000\n",
            "iteration 600 / 1500: loss 61974977315048804740907125455483551204477671067451654144.000000\n",
            "iteration 700 / 1500: loss 30501492589725865331572504514039132061914190579175379208843034624.000000\n",
            "iteration 800 / 1500: loss 15011559350344378706906074109724367172228742929345431230411876043960877056.000000\n",
            "iteration 900 / 1500: loss 7388061861760095931070612222128826917656091486276336163222663220674716630008201216.000000\n",
            "iteration 1000 / 1500: loss 3636095145035138724044742035350513253057217292696781086695639029040522378992939812712873984.000000\n",
            "iteration 1100 / 1500: loss 1789534001086227835400471965956982680338857890104914152130775556587597812639905673840039437533184000.000000\n",
            "iteration 1200 / 1500: loss 880733812869666924889037008610638145108749766504825588580197637618507994727239784868506391219778687036555264.000000\n",
            "iteration 1300 / 1500: loss 433460358205602659265264233272225439951924780278752461916427677845311255056417620291611899488376621506906753077346304.000000\n",
            "iteration 1400 / 1500: loss 213331064835060970946690729787109390924978151587466519699078134551214370675958327842686205719479026261066393412210792662564864.000000\n",
            "iteration 0 / 1500: loss 965.992088\n",
            "iteration 100 / 1500: loss 29173047354823467293075865955056222208.000000\n",
            "iteration 200 / 1500: loss 759334602426831921113022650201267369072307213336667340911204866415656960.000000\n",
            "iteration 300 / 1500: loss 19764443235217311235633950667108453724782734285116005332893871178723851762738883126130571075225306913570816.000000\n",
            "iteration 400 / 1500: loss 514441479618687623470646982135097377316497892080475415338358499540193348326982699725067351486789128883482058139303714652187470302781571596288.000000\n",
            "iteration 500 / 1500: loss 13390209519319897476128498588744175117400422795425447297175555876086374185588047957426841989691344461938509043255438389097514612737319445749643580040066852684278228690411716608.000000\n",
            "iteration 600 / 1500: loss 348528876606496826287108850384771673545178924214601513726905107736561259457976651369424851908013273873340170709847344379228143313081248910553440533387213350417615132616008162042817624572764951462890363561181184.000000\n",
            "iteration 700 / 1500: loss 9071730927982999111156376382649235721193450514360534962212085647742316402914266181290798838673575264370889809855971003434922630145103762007692644868661775026395361700521658031682345994392946298911435048531589206976366202421112044734708296187904.000000\n",
            "iteration 800 / 1500: loss 236124773450663368624150550694483948103064140806001225408613245192081603246572789021553126981602222634888286505473446223320412790369363127958606223628392224949883185947193123891005411277088349210088348569621430620681345778983717572256438285542629688758856803974010027995031404544.000000\n",
            "iteration 900 / 1500: loss inf\n",
            "iteration 1000 / 1500: loss inf\n",
            "iteration 1100 / 1500: loss inf\n",
            "iteration 1200 / 1500: loss inf\n",
            "iteration 1300 / 1500: loss inf\n",
            "iteration 1400 / 1500: loss inf\n",
            "iteration 0 / 1500: loss 1114.097644\n",
            "iteration 100 / 1500: loss 2007441707866010811746014390303699284466583830777124880384.000000\n",
            "iteration 200 / 1500: loss 3455291876848056302657135829082453674171721788462863658681779918154662234025325479299590543368807417225859301376.000000\n",
            "iteration 300 / 1500: loss 5947391601673868585853457403360031997053730704657413037801709138970249291706046796768475308711541569401674588278879319519713821634949518373470704924507776702555881472.000000\n",
            "iteration 400 / 1500: loss 10236896946583566046037981493959140085333805860788035115584969068987046349519656088214577427949352713714274059530778399614490336597018556417839792368359205917213391754718107742279459368955495397193774789031109255982219264.000000\n",
            "iteration 500 / 1500: loss 17620171347970110087460200299381916781099755124102161307706788476616031403517573439574333738189041505480264828576561101149882300547856267991278367486991427605212516647818525774893024289851490939189920711904311604339896573864627242566663496118865976551658313558609604837900288.000000\n",
            "iteration 600 / 1500: loss inf\n",
            "iteration 700 / 1500: loss inf\n",
            "iteration 800 / 1500: loss inf\n",
            "iteration 900 / 1500: loss inf\n",
            "iteration 1000 / 1500: loss inf\n",
            "iteration 1100 / 1500: loss inf\n",
            "iteration 1200 / 1500: loss nan\n",
            "iteration 1300 / 1500: loss nan\n",
            "iteration 1400 / 1500: loss nan\n",
            "iteration 0 / 1500: loss 1244.920391\n",
            "iteration 100 / 1500: loss 29246698598347956946641886737448035582156969599202218790303532767946735616.000000\n",
            "iteration 200 / 1500: loss 674841796503155646172751612548024184084735464182369758757504333614706716956045342796540007236186736042715693735236982434756403753392944397156352.000000\n",
            "iteration 300 / 1500: loss 15571379749963679205310650743364355765856429503740395688428823184430966781126353514018180515217397677773429961447213235327009748459188976101660584052696170194664750486482192706600688312860692784624765473731555360768.000000\n",
            "iteration 400 / 1500: loss 359295865451696504037894292375118149436726866195107667224640722431065304994778406478474136354693097995823276969811537718361941895655567057740575606949069378134309066279440288761978234396632319847307846163025069649954765706577488531637040854944324219859791054880046379217341275449589760.000000\n",
            "iteration 500 / 1500: loss inf\n",
            "iteration 600 / 1500: loss inf\n",
            "iteration 700 / 1500: loss inf\n",
            "iteration 800 / 1500: loss inf\n",
            "iteration 900 / 1500: loss nan\n",
            "iteration 1000 / 1500: loss nan\n",
            "iteration 1100 / 1500: loss nan\n",
            "iteration 1200 / 1500: loss nan\n",
            "iteration 1300 / 1500: loss nan\n",
            "iteration 1400 / 1500: loss nan\n",
            "iteration 0 / 1500: loss 1393.456298\n",
            "iteration 100 / 1500: loss 1286408762061751805238990145250333960160079387746021976476872159852606587849650875662336.000000\n",
            "iteration 200 / 1500: loss 1172595256499427221260564254844337496607864572338341034494424406938772767282380693352873292121226452907361160893363469431368098724797715825185005385632969386974474110238720.000000\n",
            "iteration 300 / 1500: loss 1068851267276236099417072900994239237437486002946327843191675203171570659298228107269266124704307810768076883274944473027462978800717590892091258009702964467390012860007198328291079861063924933294703745337320007756640083760196088088376768660116625294360576.000000\n",
            "iteration 400 / 1500: loss inf\n",
            "iteration 500 / 1500: loss inf\n",
            "iteration 600 / 1500: loss inf\n",
            "iteration 700 / 1500: loss inf\n",
            "iteration 800 / 1500: loss nan\n",
            "iteration 900 / 1500: loss nan\n",
            "iteration 1000 / 1500: loss nan\n",
            "iteration 1100 / 1500: loss nan\n",
            "iteration 1200 / 1500: loss nan\n",
            "iteration 1300 / 1500: loss nan\n",
            "iteration 1400 / 1500: loss nan\n",
            "iteration 0 / 1500: loss 82.874187\n",
            "iteration 100 / 1500: loss 24.084481\n",
            "iteration 200 / 1500: loss 17.054277\n",
            "iteration 300 / 1500: loss 17.260056\n",
            "iteration 400 / 1500: loss 15.016089\n",
            "iteration 500 / 1500: loss 23.051495\n",
            "iteration 600 / 1500: loss 25.716462\n",
            "iteration 700 / 1500: loss 22.240977\n",
            "iteration 800 / 1500: loss 30.308281\n",
            "iteration 900 / 1500: loss 21.254052\n",
            "iteration 1000 / 1500: loss 24.382770\n",
            "iteration 1100 / 1500: loss 21.890341\n",
            "iteration 1200 / 1500: loss 26.786578\n",
            "iteration 1300 / 1500: loss 30.545042\n",
            "iteration 1400 / 1500: loss 31.224773\n",
            "iteration 0 / 1500: loss 228.212220\n",
            "iteration 100 / 1500: loss 51.461513\n",
            "iteration 200 / 1500: loss 49.039434\n",
            "iteration 300 / 1500: loss 45.374879\n",
            "iteration 400 / 1500: loss 40.928819\n",
            "iteration 500 / 1500: loss 45.318950\n",
            "iteration 600 / 1500: loss 39.692459\n",
            "iteration 700 / 1500: loss 43.764760\n",
            "iteration 800 / 1500: loss 46.996285\n",
            "iteration 900 / 1500: loss 45.246348\n",
            "iteration 1000 / 1500: loss 51.361303\n",
            "iteration 1100 / 1500: loss 43.168150\n",
            "iteration 1200 / 1500: loss 50.138712\n",
            "iteration 1300 / 1500: loss 40.783288\n",
            "iteration 1400 / 1500: loss 38.379237\n",
            "iteration 0 / 1500: loss 381.176446\n",
            "iteration 100 / 1500: loss 94.337678\n",
            "iteration 200 / 1500: loss 73.263326\n",
            "iteration 300 / 1500: loss 82.773549\n",
            "iteration 400 / 1500: loss 85.847475\n",
            "iteration 500 / 1500: loss 70.994716\n",
            "iteration 600 / 1500: loss 74.027007\n",
            "iteration 700 / 1500: loss 80.233823\n",
            "iteration 800 / 1500: loss 76.778955\n",
            "iteration 900 / 1500: loss 72.066883\n",
            "iteration 1000 / 1500: loss 88.287419\n",
            "iteration 1100 / 1500: loss 90.125784\n",
            "iteration 1200 / 1500: loss 91.775279\n",
            "iteration 1300 / 1500: loss 82.772995\n",
            "iteration 1400 / 1500: loss 77.374062\n",
            "iteration 0 / 1500: loss 522.123093\n",
            "iteration 100 / 1500: loss 262.632984\n",
            "iteration 200 / 1500: loss 272.213946\n",
            "iteration 300 / 1500: loss 266.358524\n",
            "iteration 400 / 1500: loss 250.842276\n",
            "iteration 500 / 1500: loss 261.248378\n",
            "iteration 600 / 1500: loss 283.439427\n",
            "iteration 700 / 1500: loss 273.750151\n",
            "iteration 800 / 1500: loss 292.901448\n",
            "iteration 900 / 1500: loss 290.607392\n",
            "iteration 1000 / 1500: loss 273.240617\n",
            "iteration 1100 / 1500: loss 305.197740\n",
            "iteration 1200 / 1500: loss 282.650089\n",
            "iteration 1300 / 1500: loss 293.937259\n",
            "iteration 1400 / 1500: loss 294.339740\n",
            "iteration 0 / 1500: loss 676.356372\n",
            "iteration 100 / 1500: loss 14411.934254\n",
            "iteration 200 / 1500: loss 14892.802285\n",
            "iteration 300 / 1500: loss 14696.923932\n",
            "iteration 400 / 1500: loss 14732.838863\n",
            "iteration 500 / 1500: loss 14636.300489\n",
            "iteration 600 / 1500: loss 14108.851404\n",
            "iteration 700 / 1500: loss 14973.017106\n",
            "iteration 800 / 1500: loss 14823.343475\n",
            "iteration 900 / 1500: loss 14753.689488\n",
            "iteration 1000 / 1500: loss 14639.121264\n",
            "iteration 1100 / 1500: loss 14359.819840\n",
            "iteration 1200 / 1500: loss 14814.273164\n",
            "iteration 1300 / 1500: loss 14265.693619\n",
            "iteration 1400 / 1500: loss 14971.088496\n",
            "iteration 0 / 1500: loss 809.888899\n",
            "iteration 100 / 1500: loss 1721619042314346435387307589632.000000\n",
            "iteration 200 / 1500: loss 2737043801136083914280431425506519527355866945313551941632.000000\n",
            "iteration 300 / 1500: loss 4351374250174868356429151700610500663144047940239567511852588512317120532445218734080.000000\n",
            "iteration 400 / 1500: loss 6917849782756724911544438369441525934367558861458846588036389534505584132647884144290219868626452022176484687872.000000\n",
            "iteration 500 / 1500: loss 10998053227635856688498871638496255524703188687521678546303359332073063113616057668893710125335668202986668649695633602949997070360355274752.000000\n",
            "iteration 600 / 1500: loss 17484793482999094246986026712444093737165264588434502920088763105367758323459249096791711268878040612386816108083848180753379170080799816380656823876127752159687081984.000000\n",
            "iteration 700 / 1500: loss 27797465316399891848371936678584029313730022649057602545499570702328212701873323291033655774241776683962012845710504188789779741888947107356890444041495051838525240282859032619707198653276880896.000000\n",
            "iteration 800 / 1500: loss 44192633946049725823021809850108227104352705660559909061637915695538032221969320047737468916707347603532347514043786085428262504388671914840036933088365807613415987082836545103064077262675353867364954540461635748229021696.000000\n",
            "iteration 900 / 1500: loss 70257804906310171971673845152916672699676644496325601230790185903305757162425825616034916297005413611487629006874925045556529738044455150288908373333817060220372793565818763852874064147965014074987673405554837392681399362547561113523384591969157120.000000\n",
            "iteration 1000 / 1500: loss 111696423351438941918617871216674996837821172601472188199032536733475947542108682521059790220040652148969535296590842406234865542577094744093365811134990051427332877420311109708836441780229539508218300453798145405552196127535003515662416653629050594887647197871645388684918784.000000\n",
            "iteration 1100 / 1500: loss 177575872262746101459955695906471217313039846927612258216955158054119535868038889461124002047330552113922074314671493081896090208419025744503891102688391692854554501265703257791128806137138166988477817362801223033221245740833005642134011496443890276109139983588817439249255542204148773613794479011004416.000000\n",
            "iteration 1200 / 1500: loss inf\n",
            "iteration 1300 / 1500: loss inf\n",
            "iteration 1400 / 1500: loss inf\n",
            "iteration 0 / 1500: loss 960.068240\n",
            "iteration 100 / 1500: loss 768165717154593322149044890502476898786927582782685184.000000\n",
            "iteration 200 / 1500: loss 570680892210736882465218562236791721006934940393476605560289268749869789113323071455807385799062422814720.000000\n",
            "iteration 300 / 1500: loss 423966695546893624160933303991666269128201595058877724708980043037644398351354606185880048051243390043513134469673798734039718575053866995950323277481639936.000000\n",
            "iteration 400 / 1500: loss 314970697961578917283875203759511609586137806403375681227551148481467168451689111043475116227177523894857597305035176420989320228916698266546737403596513405288640637752021575859599933591669788463487306956800.000000\n",
            "iteration 500 / 1500: loss 233996070013077847615997240763888535250485724976354654405333020172614026540115371395828753021658051909390505956210711602672074197813418279079125086612840780321395902707601594775154706924541144181087089705779422290812027947470896016480532254559441707738333184.000000\n",
            "iteration 600 / 1500: loss 173838903542209209465530588068878957106962288552487033789498641713524417630073503668139186553828401112752211371922473575947818363454152029490814125367059025034054188240063184510517612046386159007181824706167025967722960089583636099755687691818932840877886915848737724104857079502470173098131219090795269193728.000000\n",
            "iteration 700 / 1500: loss inf\n",
            "iteration 800 / 1500: loss inf\n",
            "iteration 900 / 1500: loss inf\n",
            "iteration 1000 / 1500: loss inf\n",
            "iteration 1100 / 1500: loss inf\n",
            "iteration 1200 / 1500: loss inf\n",
            "iteration 1300 / 1500: loss nan\n",
            "iteration 1400 / 1500: loss nan\n",
            "iteration 0 / 1500: loss 1092.752463\n",
            "iteration 100 / 1500: loss 3175838827646264199631305684048937058078996270579232694585528878402371584.000000\n",
            "iteration 200 / 1500: loss 8999062032861774466677647997945630223038270356338284724592305734359409400935331948931226924700633607652867134648399626598202696317039917662208.000000\n",
            "iteration 300 / 1500: loss 25499756715083045980126966856537338974199077135237340901967549499569410364579690470041045432742141967723938334392790832635566339152928526387765189401541812420704052942321367068368994359354270012378134370525904896.000000\n",
            "iteration 400 / 1500: loss 72256151824929919592609031224860661604744250383768610910258881115284953396457005335011535588147167548407302155728821738059425743323299349167539122162310359883541136012538435882898243640258982093133838186808048838588274092736030294081011605868233765495058028913178603737680367845376.000000\n",
            "iteration 500 / 1500: loss inf\n",
            "iteration 600 / 1500: loss inf\n",
            "iteration 700 / 1500: loss inf\n",
            "iteration 800 / 1500: loss inf\n",
            "iteration 900 / 1500: loss nan\n",
            "iteration 1000 / 1500: loss nan\n",
            "iteration 1100 / 1500: loss nan\n",
            "iteration 1200 / 1500: loss nan\n",
            "iteration 1300 / 1500: loss nan\n",
            "iteration 1400 / 1500: loss nan\n",
            "iteration 0 / 1500: loss 1233.606192\n",
            "iteration 100 / 1500: loss 7054205556896497720958890971002683372679087139482689796036617183889425136233728780009472.000000\n",
            "iteration 200 / 1500: loss 39734891663617107163377179862976205778061135564962007857257522861384463191458467367463711652298194684341190838173259286596177043050055861660233105727034432522090393395789824.000000\n",
            "iteration 300 / 1500: loss 223818487111681175975349854125588904065569764281737134413486727172197813750721404996013723655818333070348768292361653122435445913761627903566900981034555938635282056426579270863464985406060680578402162239392314952562215439103507275625985411172609885061775360.000000\n",
            "iteration 400 / 1500: loss inf\n",
            "iteration 500 / 1500: loss inf\n",
            "iteration 600 / 1500: loss inf\n",
            "iteration 700 / 1500: loss inf\n",
            "iteration 800 / 1500: loss nan\n",
            "iteration 900 / 1500: loss nan\n",
            "iteration 1000 / 1500: loss nan\n",
            "iteration 1100 / 1500: loss nan\n",
            "iteration 1200 / 1500: loss nan\n",
            "iteration 1300 / 1500: loss nan\n",
            "iteration 1400 / 1500: loss nan\n",
            "iteration 0 / 1500: loss 1395.401673\n",
            "iteration 100 / 1500: loss 80029749404491754772533982825100327140666244465562101737562740582833134240482446374797332595710361600.000000\n",
            "iteration 200 / 1500: loss 4540411547636314555937122136738553193232414347706221412961763313629744990621451116290140120833304069765364269188459664607485401463188645158508233762840370422727463855496152084613731688875226403176448.000000\n",
            "iteration 300 / 1500: loss 257595921208171701443710642019084915238522352348707415532927595750734236804652769335242209223446818925685901670441192931766348826807029254671611063630996747514907463201494680555345725202652814846921599930825437369777244084198832381373769119101028509672732282700534741716135713462257507130069221376.000000\n",
            "iteration 400 / 1500: loss inf\n",
            "iteration 500 / 1500: loss inf\n",
            "iteration 600 / 1500: loss inf\n",
            "iteration 700 / 1500: loss nan\n",
            "iteration 800 / 1500: loss nan\n",
            "iteration 900 / 1500: loss nan\n",
            "iteration 1000 / 1500: loss nan\n",
            "iteration 1100 / 1500: loss nan\n",
            "iteration 1200 / 1500: loss nan\n",
            "iteration 1300 / 1500: loss nan\n",
            "iteration 1400 / 1500: loss nan\n",
            "lr 1.000000e-07 reg 2.500000e+03 train accuracy: 0.296776 val accuracy: 0.310000\n",
            "lr 1.000000e-07 reg 7.250000e+03 train accuracy: 0.348551 val accuracy: 0.365000\n",
            "lr 1.000000e-07 reg 1.200000e+04 train accuracy: 0.354551 val accuracy: 0.371000\n",
            "lr 1.000000e-07 reg 1.675000e+04 train accuracy: 0.344102 val accuracy: 0.363000\n",
            "lr 1.000000e-07 reg 2.150000e+04 train accuracy: 0.331837 val accuracy: 0.350000\n",
            "lr 1.000000e-07 reg 2.625000e+04 train accuracy: 0.328061 val accuracy: 0.349000\n",
            "lr 1.000000e-07 reg 3.100000e+04 train accuracy: 0.328959 val accuracy: 0.335000\n",
            "lr 1.000000e-07 reg 3.575000e+04 train accuracy: 0.320735 val accuracy: 0.332000\n",
            "lr 1.000000e-07 reg 4.050000e+04 train accuracy: 0.313571 val accuracy: 0.329000\n",
            "lr 1.000000e-07 reg 4.525000e+04 train accuracy: 0.307102 val accuracy: 0.324000\n",
            "lr 5.100000e-06 reg 2.500000e+03 train accuracy: 0.256245 val accuracy: 0.259000\n",
            "lr 5.100000e-06 reg 7.250000e+03 train accuracy: 0.301245 val accuracy: 0.338000\n",
            "lr 5.100000e-06 reg 1.200000e+04 train accuracy: 0.308816 val accuracy: 0.310000\n",
            "lr 5.100000e-06 reg 1.675000e+04 train accuracy: 0.224429 val accuracy: 0.225000\n",
            "lr 5.100000e-06 reg 2.150000e+04 train accuracy: 0.221163 val accuracy: 0.224000\n",
            "lr 5.100000e-06 reg 2.625000e+04 train accuracy: 0.203061 val accuracy: 0.196000\n",
            "lr 5.100000e-06 reg 3.100000e+04 train accuracy: 0.178735 val accuracy: 0.160000\n",
            "lr 5.100000e-06 reg 3.575000e+04 train accuracy: 0.175612 val accuracy: 0.179000\n",
            "lr 5.100000e-06 reg 4.050000e+04 train accuracy: 0.142531 val accuracy: 0.135000\n",
            "lr 5.100000e-06 reg 4.525000e+04 train accuracy: 0.122551 val accuracy: 0.119000\n",
            "lr 1.010000e-05 reg 2.500000e+03 train accuracy: 0.236776 val accuracy: 0.241000\n",
            "lr 1.010000e-05 reg 7.250000e+03 train accuracy: 0.177510 val accuracy: 0.196000\n",
            "lr 1.010000e-05 reg 1.200000e+04 train accuracy: 0.138429 val accuracy: 0.155000\n",
            "lr 1.010000e-05 reg 1.675000e+04 train accuracy: 0.179449 val accuracy: 0.197000\n",
            "lr 1.010000e-05 reg 2.150000e+04 train accuracy: 0.144837 val accuracy: 0.131000\n",
            "lr 1.010000e-05 reg 2.625000e+04 train accuracy: 0.133122 val accuracy: 0.151000\n",
            "lr 1.010000e-05 reg 3.100000e+04 train accuracy: 0.135837 val accuracy: 0.134000\n",
            "lr 1.010000e-05 reg 3.575000e+04 train accuracy: 0.076755 val accuracy: 0.074000\n",
            "lr 1.010000e-05 reg 4.050000e+04 train accuracy: 0.105939 val accuracy: 0.088000\n",
            "lr 1.010000e-05 reg 4.525000e+04 train accuracy: 0.076163 val accuracy: 0.074000\n",
            "lr 1.510000e-05 reg 2.500000e+03 train accuracy: 0.203776 val accuracy: 0.222000\n",
            "lr 1.510000e-05 reg 7.250000e+03 train accuracy: 0.132082 val accuracy: 0.138000\n",
            "lr 1.510000e-05 reg 1.200000e+04 train accuracy: 0.131265 val accuracy: 0.158000\n",
            "lr 1.510000e-05 reg 1.675000e+04 train accuracy: 0.129041 val accuracy: 0.125000\n",
            "lr 1.510000e-05 reg 2.150000e+04 train accuracy: 0.150633 val accuracy: 0.143000\n",
            "lr 1.510000e-05 reg 2.625000e+04 train accuracy: 0.120327 val accuracy: 0.117000\n",
            "lr 1.510000e-05 reg 3.100000e+04 train accuracy: 0.125306 val accuracy: 0.136000\n",
            "lr 1.510000e-05 reg 3.575000e+04 train accuracy: 0.118571 val accuracy: 0.096000\n",
            "lr 1.510000e-05 reg 4.050000e+04 train accuracy: 0.108163 val accuracy: 0.110000\n",
            "lr 1.510000e-05 reg 4.525000e+04 train accuracy: 0.061510 val accuracy: 0.053000\n",
            "lr 2.010000e-05 reg 2.500000e+03 train accuracy: 0.157959 val accuracy: 0.149000\n",
            "lr 2.010000e-05 reg 7.250000e+03 train accuracy: 0.135204 val accuracy: 0.136000\n",
            "lr 2.010000e-05 reg 1.200000e+04 train accuracy: 0.134694 val accuracy: 0.144000\n",
            "lr 2.010000e-05 reg 1.675000e+04 train accuracy: 0.149980 val accuracy: 0.137000\n",
            "lr 2.010000e-05 reg 2.150000e+04 train accuracy: 0.077102 val accuracy: 0.061000\n",
            "lr 2.010000e-05 reg 2.625000e+04 train accuracy: 0.062939 val accuracy: 0.057000\n",
            "lr 2.010000e-05 reg 3.100000e+04 train accuracy: 0.068224 val accuracy: 0.081000\n",
            "lr 2.010000e-05 reg 3.575000e+04 train accuracy: 0.115816 val accuracy: 0.138000\n",
            "lr 2.010000e-05 reg 4.050000e+04 train accuracy: 0.086694 val accuracy: 0.075000\n",
            "lr 2.010000e-05 reg 4.525000e+04 train accuracy: 0.080061 val accuracy: 0.078000\n",
            "lr 2.510000e-05 reg 2.500000e+03 train accuracy: 0.152163 val accuracy: 0.149000\n",
            "lr 2.510000e-05 reg 7.250000e+03 train accuracy: 0.159204 val accuracy: 0.161000\n",
            "lr 2.510000e-05 reg 1.200000e+04 train accuracy: 0.121327 val accuracy: 0.131000\n",
            "lr 2.510000e-05 reg 1.675000e+04 train accuracy: 0.086592 val accuracy: 0.102000\n",
            "lr 2.510000e-05 reg 2.150000e+04 train accuracy: 0.113204 val accuracy: 0.113000\n",
            "lr 2.510000e-05 reg 2.625000e+04 train accuracy: 0.056939 val accuracy: 0.050000\n",
            "lr 2.510000e-05 reg 3.100000e+04 train accuracy: 0.074776 val accuracy: 0.080000\n",
            "lr 2.510000e-05 reg 3.575000e+04 train accuracy: 0.123388 val accuracy: 0.136000\n",
            "lr 2.510000e-05 reg 4.050000e+04 train accuracy: 0.113694 val accuracy: 0.089000\n",
            "lr 2.510000e-05 reg 4.525000e+04 train accuracy: 0.092306 val accuracy: 0.101000\n",
            "lr 3.010000e-05 reg 2.500000e+03 train accuracy: 0.170694 val accuracy: 0.165000\n",
            "lr 3.010000e-05 reg 7.250000e+03 train accuracy: 0.120245 val accuracy: 0.104000\n",
            "lr 3.010000e-05 reg 1.200000e+04 train accuracy: 0.075245 val accuracy: 0.068000\n",
            "lr 3.010000e-05 reg 1.675000e+04 train accuracy: 0.075224 val accuracy: 0.083000\n",
            "lr 3.010000e-05 reg 2.150000e+04 train accuracy: 0.100980 val accuracy: 0.124000\n",
            "lr 3.010000e-05 reg 2.625000e+04 train accuracy: 0.067286 val accuracy: 0.078000\n",
            "lr 3.010000e-05 reg 3.100000e+04 train accuracy: 0.093265 val accuracy: 0.086000\n",
            "lr 3.010000e-05 reg 3.575000e+04 train accuracy: 0.117327 val accuracy: 0.131000\n",
            "lr 3.010000e-05 reg 4.050000e+04 train accuracy: 0.063224 val accuracy: 0.065000\n",
            "lr 3.010000e-05 reg 4.525000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
            "lr 3.510000e-05 reg 2.500000e+03 train accuracy: 0.214041 val accuracy: 0.211000\n",
            "lr 3.510000e-05 reg 7.250000e+03 train accuracy: 0.128857 val accuracy: 0.113000\n",
            "lr 3.510000e-05 reg 1.200000e+04 train accuracy: 0.085571 val accuracy: 0.088000\n",
            "lr 3.510000e-05 reg 1.675000e+04 train accuracy: 0.069143 val accuracy: 0.068000\n",
            "lr 3.510000e-05 reg 2.150000e+04 train accuracy: 0.095184 val accuracy: 0.091000\n",
            "lr 3.510000e-05 reg 2.625000e+04 train accuracy: 0.077327 val accuracy: 0.087000\n",
            "lr 3.510000e-05 reg 3.100000e+04 train accuracy: 0.078673 val accuracy: 0.078000\n",
            "lr 3.510000e-05 reg 3.575000e+04 train accuracy: 0.049673 val accuracy: 0.055000\n",
            "lr 3.510000e-05 reg 4.050000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
            "lr 3.510000e-05 reg 4.525000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
            "lr 4.010000e-05 reg 2.500000e+03 train accuracy: 0.191612 val accuracy: 0.196000\n",
            "lr 4.010000e-05 reg 7.250000e+03 train accuracy: 0.106939 val accuracy: 0.131000\n",
            "lr 4.010000e-05 reg 1.200000e+04 train accuracy: 0.103102 val accuracy: 0.114000\n",
            "lr 4.010000e-05 reg 1.675000e+04 train accuracy: 0.097204 val accuracy: 0.113000\n",
            "lr 4.010000e-05 reg 2.150000e+04 train accuracy: 0.063612 val accuracy: 0.069000\n",
            "lr 4.010000e-05 reg 2.625000e+04 train accuracy: 0.115878 val accuracy: 0.122000\n",
            "lr 4.010000e-05 reg 3.100000e+04 train accuracy: 0.129163 val accuracy: 0.134000\n",
            "lr 4.010000e-05 reg 3.575000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
            "lr 4.010000e-05 reg 4.050000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
            "lr 4.010000e-05 reg 4.525000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
            "lr 4.510000e-05 reg 2.500000e+03 train accuracy: 0.177837 val accuracy: 0.168000\n",
            "lr 4.510000e-05 reg 7.250000e+03 train accuracy: 0.120633 val accuracy: 0.119000\n",
            "lr 4.510000e-05 reg 1.200000e+04 train accuracy: 0.066020 val accuracy: 0.060000\n",
            "lr 4.510000e-05 reg 1.675000e+04 train accuracy: 0.089000 val accuracy: 0.085000\n",
            "lr 4.510000e-05 reg 2.150000e+04 train accuracy: 0.088531 val accuracy: 0.089000\n",
            "lr 4.510000e-05 reg 2.625000e+04 train accuracy: 0.117020 val accuracy: 0.098000\n",
            "lr 4.510000e-05 reg 3.100000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
            "lr 4.510000e-05 reg 3.575000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
            "lr 4.510000e-05 reg 4.050000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
            "lr 4.510000e-05 reg 4.525000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
            "best validation accuracy achieved during cross-validation: 0.371000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "test",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e798ed13-46a2-4a30-efe0-bea1b7a7996d"
      },
      "source": [
        "# evaluate on test set\n",
        "# Evaluate the best softmax on test set\n",
        "y_test_pred = best_softmax.predict(X_test)\n",
        "test_accuracy = np.mean(y_test == y_test_pred)\n",
        "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "softmax on raw pixels final test set accuracy: 0.369000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-inline"
        ],
        "id": "oVB2Ottyvmts"
      },
      "source": [
        "**Inline Question 2** - *True or False*\n",
        "\n",
        "Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
        "\n",
        "$\\color{blue}{\\textit Your Answer:}$ True\n",
        "\n",
        "\n",
        "$\\color{blue}{\\textit Your Explanation:}$ SVM에서는 각 datapoint 당 max를 계산해서 더하니 s_j + 1 < s_y_i라면 loss에 변화가 없을 수 있다. 하지만 Softmax에선 sigma(e^s_j) 항에서, 반드시 하나가 추가되고 그 값은 0 이상이므로 값이 변할 수 밖에 없다.\n",
        "다른 말로, SVM은 일정 성능의 기준을 넘으면 더이상 개선을 하지 않는데(max 함수로 구현됨) Softmax는 확률이 1이 될 때까지 계속해서 성능을 개선한다(확률을 계산함으로써 loss가 무조건적으로 변함)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf48hTIwvmts",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "1d3f7f92-2161-42ad-be2a-fdcd9c7aa3fc"
      },
      "source": [
        "# Visualize the learned weights for each class\n",
        "w = best_softmax.W[:-1,:] # strip out the bias\n",
        "w = w.reshape(32, 32, 3, 10)\n",
        "\n",
        "w_min, w_max = np.min(w), np.max(w)\n",
        "\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    \n",
        "    # Rescale the weights to be between 0 and 255\n",
        "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
        "    plt.imshow(wimg.astype('uint8'))\n",
        "    plt.axis('off')\n",
        "    plt.title(classes[i])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 10 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX5M_G_qvmtt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}